import streamlit as st
import pandas as pd
import os
import smtplib
import time
import io
import traceback
import threading
import uuid
import re
import sqlite3
import json
import random
import concurrent.futures
import tempfile
from datetime import datetime, date, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# ç¬¬ä¸‰æ–¹å¥—ä»¶
import pytz
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload

# [SRE] ç§»é™¤ PIL ä»¥é¿å… Segfaultï¼ŒçŠ§ç‰²å£“ç¸®æ›å–ç©©å®šæ€§
# from PIL import Image 

# --- 1. ç¶²é è¨­å®š ---
st.set_page_config(page_title="ä¸­å£¢å®¶å•†ï¼Œè¡›æ„›è€Œç”Ÿ", layout="wide", page_icon="ğŸ§¹")

# ==========================================
# 0. åŸºç¤è¨­å®šèˆ‡å¸¸æ•¸
# ==========================================
TW_TZ = pytz.timezone('Asia/Taipei')
MAX_IMAGE_BYTES = 15 * 1024 * 1024  # æ”¾å¯¬åˆ° 15MBï¼Œå› ç‚ºä¸å£“ç¸®äº†

# [SRE] ä½¿ç”¨ç³»çµ±æš«å­˜ç›®éŒ„
TEMP_DIR = tempfile.gettempdir()
QUEUE_DB_PATH = os.path.join(TEMP_DIR, "task_queue_v11_hybrid.db")
IMG_DIR = os.path.join(TEMP_DIR, "evidence_photos")
os.makedirs(IMG_DIR, exist_ok=True)

# Google Sheet ç¶²å€
SHEET_URL = "https://docs.google.com/spreadsheets/d/11BXtN3aevJls6Q2IR_IbT80-9XvhBkjbTCgANmsxqkg/edit"

SHEET_TABS = {
    "main": "main_data", 
    "settings": "settings",
    "roster": "roster",
    "inspectors": "inspectors",
    "duty": "duty",
    "teachers": "teachers",
    "appeals": "appeals"
}

EXPECTED_COLUMNS = [
    "æ—¥æœŸ", "é€±æ¬¡", "ç­ç´š", "è©•åˆ†é …ç›®", "æª¢æŸ¥äººå“¡",
    "å…§æƒåŸå§‹åˆ†", "å¤–æƒåŸå§‹åˆ†", "åƒåœ¾åŸå§‹åˆ†", "åƒåœ¾å…§æƒåŸå§‹åˆ†", "åƒåœ¾å¤–æƒåŸå§‹åˆ†", "æ™¨é–“æ‰“æƒåŸå§‹åˆ†", "æ‰‹æ©Ÿäººæ•¸",
    "å‚™è¨»", "é•è¦ç´°é …", "ç…§ç‰‡è·¯å¾‘", "ç™»éŒ„æ™‚é–“", "ä¿®æ­£", "æ™¨æƒæœªåˆ°è€…", "ç´€éŒ„ID"
]

APPEAL_COLUMNS = [
    "ç”³è¨´æ—¥æœŸ", "ç­ç´š", "é•è¦æ—¥æœŸ", "é•è¦é …ç›®", "åŸå§‹æ‰£åˆ†", "ç”³è¨´ç†ç”±", "ä½è­‰ç…§ç‰‡", "è™•ç†ç‹€æ…‹", "ç™»éŒ„æ™‚é–“", "å°æ‡‰ç´€éŒ„ID"
]

# ==========================================
# 1. å·¥å…·å‡½å¼
# ==========================================

def clean_id(val):
    try:
        if pd.isna(val) or val == "": return ""
        return str(int(float(val))).strip()
    except: return str(val).strip()

def execute_with_retry(func, max_retries=3, base_delay=1.0):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(base_delay + random.uniform(0, 1))
            else:
                raise e

# ==========================================
# 2. Google API é€£ç·š (åˆ†é›¢æ¨¡å¼)
# ==========================================

# --- å‰ç«¯ UI å°ˆç”¨ (æœ‰ Cache) ---
@st.cache_resource
def get_credentials_cached():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    if "gcp_service_account" not in st.secrets: return None
    creds_dict = dict(st.secrets["gcp_service_account"])
    return ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)

@st.cache_resource
def get_gspread_client():
    try:
        creds = get_credentials_cached()
        return gspread.authorize(creds) if creds else None
    except: return None

@st.cache_resource(ttl=3600)
def get_spreadsheet_object():
    client = get_gspread_client()
    try: return client.open_by_url(SHEET_URL) if client else None
    except: return None

# --- èƒŒæ™¯ Worker å°ˆç”¨ (ç„¡ Cacheï¼Œé¿å… Context Error) ---
def get_raw_sheet_client():
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds_dict = dict(st.secrets["gcp_service_account"])
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        return gspread.authorize(creds)
    except: return None

def get_raw_drive_service():
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds_dict = dict(st.secrets["gcp_service_account"])
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        return build('drive', 'v3', credentials=creds, cache_discovery=False)
    except: return None

# ==========================================
# 3. åœ–ç‰‡ä¸Šå‚³ (å‰æ™¯å¹³è¡ŒåŒ–)
# ==========================================

def upload_single_image(args):
    """å–®å¼µåœ–ç‰‡ä¸Šå‚³é‚è¼¯"""
    file_bytes, filename = args
    service = get_raw_drive_service() # æ¯å€‹ Thread è‡ªå·±æ‹¿é€£ç·š
    if not service: return None
    
    folder_id = st.secrets["system_config"].get("drive_folder_id")
    
    try:
        metadata = {'name': filename}
        if folder_id: metadata['parents'] = [folder_id]
        
        media = MediaIoBaseUpload(io.BytesIO(file_bytes), mimetype='image/jpeg', resumable=True)
        file = service.files().create(body=metadata, media_body=media, fields='id,webViewLink').execute()
        return file.get('webViewLink') or f"https://drive.google.com/file/d/{file.get('id')}/view"
    except Exception as e:
        print(f"[Upload Error] {e}")
        return None

def upload_images_hybrid(files_list, entry_data):
    """
    [Hybrid Mode] ä½¿ç”¨ ThreadPoolExecutor(max_workers=2)
    åœ¨å‰æ™¯åŸ·è¡Œï¼Œç¢ºä¿æ‹¿åˆ°é€£çµæ‰ç¹¼çºŒï¼Œä½†æ¯”å–®åŸ·è¡Œç·’å¿«ã€‚
    """
    if not files_list: return [], True

    tasks = []
    for i, up_file in enumerate(files_list):
        up_file.seek(0)
        raw = up_file.read() # è®€å–ç‚º Bytes
        
        safe_class = str(entry_data.get("ç­ç´š", "unknown"))
        logical_fname = f"{entry_data.get('æ—¥æœŸ', '')}_{safe_class}_{i}.jpg"
        unique_prefix = f"{datetime.now(TW_TZ).strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:8]}"
        drive_filename = f"{unique_prefix}_{logical_fname}"
        
        tasks.append((raw, drive_filename))

    uploaded_links = [None] * len(tasks)
    
    # ä½¿ç”¨ 2 å€‹ Worker å¹³è¡Œä¸Šå‚³ (æ¯” 4 å€‹å®‰å…¨ï¼Œæ¯” 1 å€‹å¿«)
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_idx = {executor.submit(upload_single_image, task): i for i, task in enumerate(tasks)}
        
        for future in concurrent.futures.as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                link = future.result()
                uploaded_links[idx] = link
            except:
                uploaded_links[idx] = None

    if any(l is None for l in uploaded_links):
        return [], False # åš´æ ¼æ¨¡å¼ï¼šåªè¦æœ‰ä¸€å¼µå¤±æ•—å°±å…¨æ“‹
    
    return uploaded_links, True

# ==========================================
# 4. SQLite èƒŒæ™¯ä½‡åˆ— (Queue) - å¾©æ´» SRE é¢æ¿
# ==========================================

_db_lock = threading.Lock()

def get_db_connection():
    # æ¯æ¬¡é€£ç·šï¼Œç¢ºä¿ Thread Safe
    try:
        conn = sqlite3.connect(QUEUE_DB_PATH, check_same_thread=False, timeout=30.0, isolation_level=None)
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA busy_timeout=30000;")
        return conn
    except: return None

def init_db():
    with _db_lock:
        conn = get_db_connection()
        if conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_queue (
                    id TEXT PRIMARY KEY,
                    task_type TEXT NOT NULL,
                    created_ts TEXT NOT NULL,
                    payload_json TEXT NOT NULL,
                    status TEXT NOT NULL,
                    attempts INTEGER NOT NULL DEFAULT 0,
                    last_error TEXT
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_status ON task_queue (status);")
            conn.close()

init_db()

def enqueue_task(task_type, payload):
    # å°‡è³‡æ–™å¯«å…¥ SQLite (æ¥µå¿«ï¼Œä¸æœƒå¡ UI)
    task_id = str(uuid.uuid4())
    now = datetime.utcnow().isoformat() + "Z"
    js = json.dumps(payload, ensure_ascii=False)
    
    with _db_lock:
        conn = get_db_connection()
        if conn:
            try:
                conn.execute("BEGIN IMMEDIATE")
                conn.execute(
                    "INSERT INTO task_queue (id, task_type, created_ts, payload_json, status) VALUES (?, ?, ?, ?, 'PENDING')",
                    (task_id, task_type, now, js)
                )
                conn.commit()
                conn.close()
                ensure_worker_started() # å–šé†’ Worker
                return True
            except: 
                conn.close()
    return False

def get_queue_metrics():
    # SRE é¢æ¿éœ€è¦çš„æ•¸æ“š
    metrics = {"pending": 0, "processed": 0, "failed": 0}
    with _db_lock:
        conn = get_db_connection()
        if conn:
            try:
                cur = conn.cursor()
                cur.execute("SELECT status, COUNT(*) FROM task_queue GROUP BY status")
                for s, c in cur.fetchall():
                    if s == 'PENDING': metrics["pending"] = c
                    elif s == 'DONE': metrics["processed"] = c
                    elif s == 'FAILED': metrics["failed"] = c
                conn.close()
            except: pass
    return metrics

# --- èƒŒæ™¯ Worker (ç´”æ·¨ç‰ˆï¼šä¸ç¢° Streamlit Context) ---

def worker_loop(stop_event):
    print("ğŸš€ Background Worker Started")
    client = get_raw_sheet_client() # é å…ˆå»ºç«‹é€£ç·š
    
    while not stop_event.is_set():
        task = None
        conn = get_db_connection()
        if not conn:
            time.sleep(5)
            continue

        # 1. é ˜å–ä»»å‹™ (Atomic Claim)
        try:
            with _db_lock:
                conn.execute("BEGIN IMMEDIATE")
                cur = conn.cursor()
                cur.execute("SELECT id, task_type, payload_json FROM task_queue WHERE status='PENDING' LIMIT 1")
                row = cur.fetchone()
                if row:
                    task = row
                    conn.execute("UPDATE task_queue SET status='RUNNING' WHERE id=?", (task[0],))
                    conn.commit()
                else:
                    conn.commit()
        except: pass
        finally: conn.close()

        if not task:
            time.sleep(2) # æ²’äº‹åšå°±ä¼‘æ¯
            continue

        # 2. åŸ·è¡Œä»»å‹™ (å¯«å…¥ Sheet)
        t_id, t_type, t_payload = task
        try:
            data = json.loads(t_payload)
            entry = data.get("entry")
            
            # é€™è£¡é‡æ–°å–å¾— client ä»¥é˜²éæœŸï¼Œä¸¦å…·å‚™é‡è©¦æ©Ÿåˆ¶
            local_client = get_raw_sheet_client()
            sheet = local_client.open_by_url(SHEET_URL)
            
            target_tab = SHEET_TABS["main"] if t_type == "main_entry" else SHEET_TABS["appeals"]
            try:
                ws = sheet.worksheet(target_tab)
            except:
                ws = sheet.add_worksheet(target_tab, 100, 20)
                # è£œè¡¨é ­
                header = EXPECTED_COLUMNS if t_type == "main_entry" else APPEAL_COLUMNS
                ws.append_row(header)

            # æº–å‚™ Row
            row_vals = []
            cols = EXPECTED_COLUMNS if t_type == "main_entry" else APPEAL_COLUMNS
            for col in cols:
                val = entry.get(col, "")
                if isinstance(val, bool): val = str(val).upper()
                row_vals.append(val)
            
            ws.append_row(row_vals)
            
            # 3. æ¨™è¨˜å®Œæˆ
            with _db_lock:
                c2 = get_db_connection()
                c2.execute("UPDATE task_queue SET status='DONE' WHERE id=?", (t_id,))
                c2.commit()
                c2.close()
            print(f"âœ… Task {t_id} Done")

        except Exception as e:
            print(f"âŒ Task {t_id} Failed: {e}")
            with _db_lock:
                c3 = get_db_connection()
                c3.execute("UPDATE task_queue SET status='FAILED', last_error=? WHERE id=?", (str(e), t_id))
                c3.commit()
                c3.close()

# --- ä¸æ­»é³¥æ©Ÿåˆ¶ ---
_worker_thread = None
def ensure_worker_started():
    global _worker_thread
    if _worker_thread is None or not _worker_thread.is_alive():
        stop_ev = threading.Event()
        _worker_thread = threading.Thread(target=worker_loop, args=(stop_ev,), daemon=True)
        _worker_thread.start()

ensure_worker_started()

# ==========================================
# 5. å‰ç«¯è®€å–èˆ‡ UI
# ==========================================

@st.cache_data(ttl=60)
def load_main_data():
    ws_obj = get_worksheet(SHEET_TABS["main"]) # ä½¿ç”¨ cached helper
    if not ws_obj: return pd.DataFrame(columns=EXPECTED_COLUMNS)
    try:
        return pd.DataFrame(ws_obj.get_all_records())
    except: return pd.DataFrame(columns=EXPECTED_COLUMNS)

def get_worksheet(tab_name):
    # ç‚ºäº† UI è®€å–æ–¹ä¾¿çš„ Helper
    sheet = get_spreadsheet_object()
    if not sheet: return None
    try: return sheet.worksheet(tab_name)
    except: return None

# ==========================================
# 6. ä¸»ç¨‹å¼ä»‹é¢
# ==========================================

# è®€å–å¿…è¦è¨­å®š
all_classes, _ = load_sorted_classes()
if not all_classes: all_classes = ["æ¸¬è©¦ç­ç´š"]

st.sidebar.title("ğŸ« è©•åˆ†ç³»çµ± (Hybrid Pro)")
app_mode = st.sidebar.radio("æ¨¡å¼", ["è©•åˆ†è¼¸å…¥", "è³‡æ–™æŸ¥è©¢", "å¾Œå°ç›£æ§"])

if app_mode == "è©•åˆ†è¼¸å…¥":
    st.title("ğŸ“ è©•åˆ†è¼¸å…¥")
    pwd = st.text_input("é€šè¡Œç¢¼", type="password")
    if pwd == st.secrets["system_config"]["team_password"]:
        
        c1, c2 = st.columns(2)
        d_input = c1.date_input("æ—¥æœŸ", date.today())
        insp = c2.text_input("æª¢æŸ¥äººå“¡", "è¡›ç”Ÿçµ„")
        cls = st.selectbox("ç­ç´š", all_classes)
        role = st.radio("é …ç›®", ["å…§æƒæª¢æŸ¥", "å¤–æƒæª¢æŸ¥", "åƒåœ¾æª¢æŸ¥"])
        
        with st.form("main_form"):
            score = st.number_input("æ‰£åˆ†", min_value=0, step=1)
            note = st.text_input("èªªæ˜")
            files = st.file_uploader("ç…§ç‰‡ (æœ€å¤š4å¼µ)", accept_multiple_files=True)
            
            if st.form_submit_button("é€å‡º"):
                entry = {
                    "æ—¥æœŸ": str(d_input),
                    "é€±æ¬¡": get_week_num(d_input),
                    "ç­ç´š": cls,
                    "è©•åˆ†é …ç›®": role,
                    "æª¢æŸ¥äººå“¡": insp,
                    "å…§æƒåŸå§‹åˆ†": score if role=="å…§æƒæª¢æŸ¥" else 0,
                    "å¤–æƒåŸå§‹åˆ†": score if role=="å¤–æƒæª¢æŸ¥" else 0,
                    "åƒåœ¾åŸå§‹åˆ†": score if role=="åƒåœ¾æª¢æŸ¥" else 0,
                    "å‚™è¨»": note,
                    "ç™»éŒ„æ™‚é–“": datetime.now(TW_TZ).strftime("%Y-%m-%d %H:%M:%S"),
                    "ç´€éŒ„ID": f"{datetime.now(TW_TZ).strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:6]}"
                }
                
                # 1. åš´æ ¼æ¨¡å¼ï¼šç…§ç‰‡å¿…é ˆå…ˆä¸Šå‚³æˆåŠŸ (å‰æ™¯å¹³è¡Œè™•ç†)
                if files:
                    if len(files) > 4:
                        st.error("âŒ ç…§ç‰‡éå¤š")
                        st.stop()
                    
                    with st.spinner("â˜ï¸ æ­£åœ¨æ¥µé€Ÿä¸Šå‚³ç…§ç‰‡ (é©—è­‰ä¸­)..."):
                        links, ok = upload_images_hybrid(files, entry)
                        if not ok:
                            st.error("âŒ ç…§ç‰‡ä¸Šå‚³å¤±æ•—ï¼Œç‚ºä¿å…¨è­‰æ“šï¼Œæœ¬ç­†è³‡æ–™æœªé€å‡ºã€‚")
                            st.stop()
                        entry["ç…§ç‰‡è·¯å¾‘"] = ";".join(links)
                
                # 2. è³‡æ–™å¯«å…¥ï¼šä¸Ÿçµ¦èƒŒæ™¯ä½‡åˆ— (ç§’å›)
                if enqueue_task("main_entry", {"entry": entry}):
                    st.success("âœ… è³‡æ–™å·²æ’å…¥ä½‡åˆ—ï¼Œå°‡è‡ªå‹•å¯«å…¥è©¦ç®—è¡¨ï¼")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("âŒ ç³»çµ±ç¹å¿™ (DB Locked)ï¼Œè«‹é‡è©¦")

elif app_mode == "å¾Œå°ç›£æ§":
    st.title("ğŸ“¡ SRE ç›£æ§é¢æ¿")
    metrics = get_queue_metrics()
    c1, c2, c3 = st.columns(3)
    c1.metric("å¾…è™•ç† (Pending)", metrics["pending"])
    c2.metric("å·²å®Œæˆ (Done)", metrics["processed"])
    c3.metric("å¤±æ•— (Failed)", metrics["failed"])
    
    if st.button("æ‰‹å‹•å–šé†’ Worker"):
        ensure_worker_started()
        st.toast("å·²ç™¼é€å–šé†’è¨Šè™Ÿ")

elif app_mode == "è³‡æ–™æŸ¥è©¢":
    st.title("ğŸ“Š è³‡æ–™æŸ¥è©¢")
    df = load_main_data()
    st.dataframe(df)
