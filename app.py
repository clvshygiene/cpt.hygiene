import streamlit as st
import pandas as pd
import os
import smtplib
import time
import io
import traceback
import threading
import uuid
import re
import sqlite3
import json
import random
import concurrent.futures
import tempfile
from datetime import datetime, date, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# ç¬¬ä¸‰æ–¹å¥—ä»¶
import pytz
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload

# --- 1. ç¶²é è¨­å®š ---
st.set_page_config(page_title="ä¸­å£¢å®¶å•†ï¼Œè¡›æ„›è€Œç”Ÿ", layout="wide", page_icon="ğŸ§¹")

# ==========================================
# 0. åŸºç¤è¨­å®šèˆ‡å¸¸æ•¸
# ==========================================
TW_TZ = pytz.timezone('Asia/Taipei')
MAX_IMAGE_BYTES = 10 * 1024 * 1024  # å–®æª”åœ–ç‰‡ 10MB ä¸Šé™

# [SRE] ä½¿ç”¨ç³»çµ±æš«å­˜ç›®éŒ„ï¼Œé©æ‡‰ Streamlit Cloud Ephemeral ç’°å¢ƒ
TEMP_DIR = tempfile.gettempdir()
QUEUE_DB_PATH = os.path.join(TEMP_DIR, "task_queue_v9_threadsafe.db") # Version updated
IMG_DIR = os.path.join(TEMP_DIR, "evidence_photos")
os.makedirs(IMG_DIR, exist_ok=True)

# Google Sheet ç¶²å€
SHEET_URL = "https://docs.google.com/spreadsheets/d/11BXtN3aevJls6Q2IR_IbT80-9XvhBkjbTCgANmsxqkg/edit"

SHEET_TABS = {
    "main": "main_data", 
    "settings": "settings",
    "roster": "roster",
    "inspectors": "inspectors",
    "duty": "duty",
    "teachers": "teachers",
    "appeals": "appeals"
}

EXPECTED_COLUMNS = [
    "æ—¥æœŸ", "é€±æ¬¡", "ç­ç´š", "è©•åˆ†é …ç›®", "æª¢æŸ¥äººå“¡",
    "å…§æƒåŸå§‹åˆ†", "å¤–æƒåŸå§‹åˆ†", "åƒåœ¾åŸå§‹åˆ†", "åƒåœ¾å…§æƒåŸå§‹åˆ†", "åƒåœ¾å¤–æƒåŸå§‹åˆ†", "æ™¨é–“æ‰“æƒåŸå§‹åˆ†", "æ‰‹æ©Ÿäººæ•¸",
    "å‚™è¨»", "é•è¦ç´°é …", "ç…§ç‰‡è·¯å¾‘", "ç™»éŒ„æ™‚é–“", "ä¿®æ­£", "æ™¨æƒæœªåˆ°è€…", "ç´€éŒ„ID"
]

APPEAL_COLUMNS = [
    "ç”³è¨´æ—¥æœŸ", "ç­ç´š", "é•è¦æ—¥æœŸ", "é•è¦é …ç›®", "åŸå§‹æ‰£åˆ†", "ç”³è¨´ç†ç”±", "ä½è­‰ç…§ç‰‡", "è™•ç†ç‹€æ…‹", "ç™»éŒ„æ™‚é–“", "å°æ‡‰ç´€éŒ„ID"
]

# ==========================================
# 1. å·¥å…·å‡½å¼ (Utils)
# ==========================================

def compress_image_bytes(raw: bytes, max_side: int = 1600, quality: int = 75) -> bytes:
    """
    [SRE] Pass-through æ¨¡å¼ï¼Œé¿å… PIL C-extension åœ¨é›²ç«¯ç’°å¢ƒ Crashã€‚
    """
    return raw

def clean_id(val):
    try:
        if pd.isna(val) or val == "": return ""
        return str(int(float(val))).strip()
    except: return str(val).strip()

def execute_with_retry(func, max_retries=5, base_delay=1.0):
    """
    SRE Pattern: æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶
    """
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            error_str = str(e).lower()
            is_retryable = any(x in error_str for x in ['429', '500', '503', 'quota', 'rate limit', 'timed out', 'connection'])
            
            if is_retryable and attempt < max_retries - 1:
                sleep_time = (base_delay * (2 ** attempt)) + random.uniform(0, 1)
                print(f"âš ï¸ API å¿™ç¢Œ ({e})ï¼Œç¬¬ {attempt+1} æ¬¡é‡è©¦ï¼Œç­‰å¾… {sleep_time:.2f}ç§’...")
                time.sleep(sleep_time)
            else:
                raise e

# ==========================================
# 2. Google API é€£ç·š (Thread-Safe Fix)
# ==========================================

# [SRE Fix] é€™æ˜¯åŸæœ¬çš„ Cache ç‰ˆæœ¬ï¼Œçµ¦ UI é¡¯ç¤ºç”¨ (è®€å– Sheet)
@st.cache_resource
def get_credentials_cached():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    if "gcp_service_account" not in st.secrets:
        return None
    creds_dict = dict(st.secrets["gcp_service_account"])
    return ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)

@st.cache_resource
def get_gspread_client():
    try:
        creds = get_credentials_cached()
        if not creds: return None
        return gspread.authorize(creds)
    except Exception as e:
        st.error(f"âŒ Google Sheet é€£ç·šå¤±æ•—: {e}")
        return None

@st.cache_resource(ttl=3600)
def get_spreadsheet_object():
    client = get_gspread_client()
    if not client: return None
    try: return client.open_by_url(SHEET_URL)
    except Exception as e: 
        st.error(f"âŒ ç„¡æ³•é–‹å•Ÿè©¦ç®—è¡¨: {e}")
        return None

# [SRE Fix] é€™æ˜¯ã€Œç„¡å¿«å–ã€ç‰ˆæœ¬ï¼Œå°ˆé–€çµ¦ Thread ä½¿ç”¨
# é¿å…åœ¨ Thread ä¸­å‘¼å« st.cache_resource å°è‡´ Missing ScriptRunContext
def get_drive_service_raw():
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        if "gcp_service_account" not in st.secrets:
            return None
        creds_dict = dict(st.secrets["gcp_service_account"])
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        return build('drive', 'v3', credentials=creds, cache_discovery=False)
    except Exception as e:
        print(f"Drive Service Error: {e}")
        return None

def get_worksheet(tab_name):
    max_retries = 3
    wait_time = 2
    sheet = get_spreadsheet_object()
    if not sheet: return None
    for attempt in range(max_retries):
        try:
            try: return sheet.worksheet(tab_name)
            except gspread.WorksheetNotFound:
                cols = 20 if tab_name != "appeals" else 15
                ws = sheet.add_worksheet(title=tab_name, rows=100, cols=cols)
                if tab_name == "appeals": ws.append_row(APPEAL_COLUMNS)
                return ws
        except Exception as e:
            if "429" in str(e): 
                time.sleep(wait_time * (attempt + 1))
                continue
            else: 
                print(f"âŒ è®€å–åˆ†é  '{tab_name}' å¤±æ•—: {e}")
                return None
    return None

def upload_image_to_drive(file_obj, filename):
    # [SRE Fix] é€™è£¡æ”¹ç”¨ raw serviceï¼Œä¸ä½¿ç”¨ Streamlit Cache
    service = get_drive_service_raw()
    if not service: 
        print("âŒ Drive Service Not Available (Raw)")
        return None
    
    folder_id = None
    if "system_config" in st.secrets and "drive_folder_id" in st.secrets["system_config"]:
        folder_id = st.secrets["system_config"]["drive_folder_id"]

    def _upload_action():
        metadata = {'name': filename}
        if folder_id:
            metadata['parents'] = [folder_id]
            
        media = MediaIoBaseUpload(file_obj, mimetype='image/jpeg', resumable=True)
        file = service.files().create(body=metadata, media_body=media, fields='id,webViewLink').execute()
        return file.get('webViewLink') or f"https://drive.google.com/file/d/{file.get('id')}/view"

    try:
        return execute_with_retry(_upload_action)
    except Exception as e:
        print(f"âš ï¸ Drive ä¸Šå‚³æœ€çµ‚å¤±æ•—: {str(e)}")
        return None

def upload_images_parallel(files_list, entry_data):
    if not files_list:
        return [], True

    upload_results = [None] * len(files_list)
    tasks = []
    
    for i, up_file in enumerate(files_list):
        up_file.seek(0)
        raw = up_file.read()
        data = compress_image_bytes(raw)

        safe_class = str(entry_data.get("ç­ç´š", "unknown"))
        logical_fname = f"{entry_data.get('æ—¥æœŸ', '')}_{safe_class}_{i}.jpg"
        unique_prefix = f"{datetime.now(TW_TZ).strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:8]}"
        drive_filename = f"{unique_prefix}_{logical_fname}"
        
        tasks.append((io.BytesIO(data), drive_filename, i))

    # [SRE] ä¸¦è¡Œæ•¸é™åˆ¶ 2
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_index = {
            executor.submit(upload_image_to_drive, f_obj, fname): idx 
            for f_obj, fname, idx in tasks
        }
        
        for future in concurrent.futures.as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                link = future.result()
                upload_results[idx] = link
            except Exception as e:
                print(f"ä¸Šå‚³å¤±æ•—: {e}")
                upload_results[idx] = None

    if any(link is None for link in upload_results):
        return [], False
    
    return upload_results, True

# ==========================================
# 3. SQLite èƒŒæ™¯ä½‡åˆ—ç³»çµ± (SRE Hardened v4 - Production Safe)
# ==========================================

_db_lock = threading.Lock()

def get_new_db_connection():
    try:
        conn = sqlite3.connect(
            QUEUE_DB_PATH, 
            check_same_thread=False, 
            timeout=30.0, 
            isolation_level=None
        )
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA busy_timeout=30000;")
        conn.execute("PRAGMA synchronous=NORMAL;")
        return conn
    except Exception as e:
        print(f"[CRITICAL] DB Connect Error: {e}")
        return None

def init_db_if_needed():
    with _db_lock:
        conn = get_new_db_connection()
        if conn:
            try:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS task_queue (
                        id TEXT PRIMARY KEY,
                        task_type TEXT NOT NULL,
                        created_ts TEXT NOT NULL,
                        payload_json TEXT NOT NULL,
                        status TEXT NOT NULL,
                        attempts INTEGER NOT NULL DEFAULT 0,
                        last_error TEXT
                    )
                """)
                conn.execute("CREATE INDEX IF NOT EXISTS idx_status_created ON task_queue (status, created_ts);")
                conn.close()
            except Exception as e:
                print(f"[INIT ERROR] {e}")

init_db_if_needed()

def recover_stale_tasks():
    with _db_lock:
        conn = get_new_db_connection()
        if not conn: return
        try:
            conn.execute("BEGIN IMMEDIATE")
            cur = conn.cursor()
            cur.execute(
                "UPDATE task_queue SET status='RETRY', last_error='Worker Restarted Recovery', attempts=attempts WHERE status='RUNNING'"
            )
            count = cur.rowcount
            conn.commit()
            if count > 0:
                print(f"â™»ï¸  å·²å¾©åŸ {count} ç­†æ®­å±ä»»å‹™")
        except Exception as e:
            print(f"[RECOVERY ERROR] {e}")
        finally:
            conn.close()

def enqueue_task(task_type: str, payload: dict) -> str:
    ensure_worker_started()
    
    task_id = str(uuid.uuid4())
    created_ts = datetime.utcnow().isoformat() + "Z"
    payload_json = json.dumps(payload, ensure_ascii=False)

    with _db_lock:
        conn = get_new_db_connection()
        if not conn: return "DB_ERROR"
        try:
            conn.execute("BEGIN IMMEDIATE")
            conn.execute(
                "INSERT INTO task_queue (id, task_type, created_ts, payload_json, status, attempts, last_error) "
                "VALUES (?, ?, ?, ?, 'PENDING', 0, NULL)",
                (task_id, task_type, created_ts, payload_json)
            )
            conn.commit()
            return task_id
        except Exception as e:
            try: conn.rollback()
            except: pass
            print(f"[ERROR] Enqueue Failed: {e}")
            return "DB_ERROR"
        finally:
            conn.close()

def get_queue_metrics():
    metrics = {"pending": 0, "retry": 0, "failed": 0, "running": 0, "oldest_pending_sec": 0, "recent_errors": []}
    with _db_lock:
        conn = get_new_db_connection()
        if not conn: return metrics
        try:
            cur = conn.cursor()
            cur.execute("SELECT status, COUNT(*) FROM task_queue GROUP BY status")
            rows = cur.fetchall()
            for status, count in rows:
                if status == 'PENDING': metrics["pending"] = count
                elif status == 'RETRY': metrics["retry"] = count
                elif status == 'FAILED': metrics["failed"] = count
                elif status == 'RUNNING': metrics["running"] = count
            
            cur.execute("SELECT MIN(created_ts) FROM task_queue WHERE status IN ('PENDING', 'RETRY', 'RUNNING')")
            row = cur.fetchone()
            oldest_ts_str = row[0] if row else None
            
            cur.execute("SELECT last_error, created_ts FROM task_queue WHERE status='FAILED' OR status='RETRY' ORDER BY created_ts DESC LIMIT 5")
            metrics["recent_errors"] = cur.fetchall()
            
            if oldest_ts_str:
                try:
                    created = datetime.fromisoformat(oldest_ts_str.replace("Z", "+00:00"))
                    now = datetime.now(pytz.utc)
                    metrics["oldest_pending_sec"] = (now - created).total_seconds()
                except: pass
        except Exception as e: 
            print(f"[WARN] Metrics Error: {e}")
        finally:
            conn.close()
    return metrics

def fetch_next_task(max_attempts: int = 6):
    with _db_lock:
        conn = get_new_db_connection()
        if not conn: return None
        try:
            conn.execute("BEGIN IMMEDIATE")
            cur = conn.cursor()
            
            cur.execute(
                """
                SELECT id, task_type, created_ts, payload_json, status, attempts
                FROM task_queue
                WHERE status IN ('PENDING', 'RETRY')
                AND attempts < ?
                ORDER BY created_ts ASC
                LIMIT 1
                """,
                (max_attempts,)
            )
            row = cur.fetchone()
            
            if not row:
                conn.commit()
                return None

            task_id, task_type, created_ts, payload_json, status, attempts = row
            new_attempts = attempts + 1
            
            # [SRE Fix] é˜²ç¦¦æ€§æ›´æ–°
            cur.execute(
                "UPDATE task_queue SET status = 'RUNNING', attempts = ? WHERE id = ? AND status IN ('PENDING', 'RETRY')",
                (new_attempts, task_id)
            )
            
            if cur.rowcount == 0:
                conn.rollback()
                return None
            
            conn.commit()
            
            try: payload = json.loads(payload_json)
            except: payload = {}
                
            return {
                "id": task_id,
                "task_type": task_type,
                "created_ts": created_ts,
                "payload": payload,
                "attempts": new_attempts,
            }
        except Exception as e:
            try: conn.rollback()
            except: pass
            print(f"[ERROR] Fetch Task Error: {e}")
            return None
        finally:
            conn.close()

def update_task_status(task_id: str, status: str, last_error: str | None):
    with _db_lock:
        conn = get_new_db_connection()
        if not conn: return
        try:
            conn.execute("BEGIN IMMEDIATE")
            conn.execute(
                "UPDATE task_queue SET status = ?, last_error = ? WHERE id = ?",
                (status, last_error, task_id),
            )
            conn.commit()
        except Exception as e: 
            try: conn.rollback()
            except: pass
            print(f"[ERROR] Update Status Failed ({task_id}): {e}")
        finally:
            conn.close()

# --- Worker Logic ---

def _append_main_entry_row(entry: dict):
    def _action():
        ws = get_worksheet(SHEET_TABS["main"])
        if not ws: raise Exception("Failed to get main worksheet")
        
        all_vals = ws.get_all_values()
        if not all_vals: ws.append_row(EXPECTED_COLUMNS)

        row = []
        for col in EXPECTED_COLUMNS:
            val = entry.get(col, "")
            if isinstance(val, bool): val = str(val).upper()
            if col == "æ—¥æœŸ": val = str(val)
            row.append(val)
        ws.append_row(row)
    execute_with_retry(_action)

def _append_appeal_row(entry: dict):
    def _action():
        ws = get_worksheet(SHEET_TABS["appeals"])
        if not ws: raise Exception("Failed to get appeals worksheet")

        all_vals = ws.get_all_values()
        if not all_vals: ws.append_row(APPEAL_COLUMNS)

        row = [str(entry.get(col, "")) for col in APPEAL_COLUMNS]
        ws.append_row(row)
    execute_with_retry(_action)

def process_task(task: dict) -> tuple[bool, str | None]:
    task_type = task["task_type"]
    payload = task["payload"]
    entry = payload.get("entry", {}) or {}

    try:
        if task_type == "main_entry":
            _append_main_entry_row(entry)
            return True, None

        elif task_type == "appeal_entry":
            image_info = payload.get("image_file")
            if image_info and image_info.get("path") and os.path.exists(image_info["path"]):
                with open(image_info["path"], "rb") as f:
                    link = upload_image_to_drive(f, image_info["filename"])
                entry["ä½è­‰ç…§ç‰‡"] = link if link else "UPLOAD_FAILED"
            else:
                entry["ä½è­‰ç…§ç‰‡"] = entry.get("ä½è­‰ç…§ç‰‡", "")

            _append_appeal_row(entry)
            return True, None
        else:
            return True, None

    except Exception as e:
        return False, str(e)

def process_task_wrapper(task, max_attempts):
    task_id = task["id"]
    attempts = task["attempts"] 
    payload = task["payload"]
    
    ok = False
    err_msg = None
    try:
        ok, err_msg = process_task(task)
    except Exception as e:
        err_msg = f"UNHANDLED: {e}\n{traceback.format_exc()}"
        ok = False

    try:
        image_paths = []
        if isinstance(payload, dict):
            if "image_file" in payload and "path" in payload["image_file"]:
                image_paths.append(payload["image_file"]["path"])
        for p in image_paths:
            if p and os.path.exists(p): os.remove(p)
    except: pass

    if ok:
        update_task_status(task_id, "DONE", None)
        print(f"âœ… Task {task_id} å®Œæˆ")
    else:
        status = "FAILED" if attempts >= max_attempts else "RETRY"
        print(f"âš ï¸ Task {task_id} å¤±æ•—: {err_msg}")
        update_task_status(task_id, status, err_msg or "unknown")

def background_worker(stop_event: threading.Event | None = None):
    recover_stale_tasks()
    
    # [SRE] èƒŒæ™¯ Worker ä»ç„¶ä½¿ç”¨ ThreadPool æ˜¯å®‰å…¨çš„ï¼Œå› ç‚ºå®ƒå€‘ä¸æ¶‰åŠ Streamlit Context
    max_attempts = 6
    MAX_WORKERS = 2 
    print(f"ğŸš€ èƒŒæ™¯å·¥ä½œè€…å•Ÿå‹• (Workers: {MAX_WORKERS})...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        
        while True:
            if stop_event is not None and stop_event.is_set():
                break

            done_futures = [f for f in futures if f.done()]
            for f in done_futures:
                futures.remove(f)
                try: f.result() 
                except Exception as e: print(f"[ERROR] Thread Future Error: {e}")

            if len(futures) >= MAX_WORKERS:
                time.sleep(0.5)
                continue

            task = fetch_next_task(max_attempts=max_attempts)
            if not task:
                time.sleep(2.0) 
                continue

            print(f"âš¡ ä»»å‹™ {task['id']} (Try: {task['attempts']}) åŸ·è¡Œä¸­")
            future = executor.submit(process_task_wrapper, task, max_attempts)
            futures.append(future)

# --- Phoenix Keeper ---
_worker_lock = threading.Lock()
_worker_thread = None
_worker_stop_event = None

def ensure_worker_started():
    global _worker_thread, _worker_stop_event
    with _worker_lock:
        if _worker_thread is None or not _worker_thread.is_alive():
            print("â¤ï¸â€ğŸ”¥ å•Ÿå‹•/é‡å•ŸèƒŒæ™¯å·¥ä½œè€…...")
            _worker_stop_event = threading.Event()
            _worker_thread = threading.Thread(target=background_worker, args=(_worker_stop_event,), daemon=True)
            _worker_thread.start()

# ==========================================
# 4. è³‡æ–™è®€å–é‚è¼¯ (Frontend)
# ==========================================

@st.cache_data(ttl=60)
def load_main_data():
    ws = get_worksheet(SHEET_TABS["main"])
    if not ws:
        return pd.DataFrame(columns=EXPECTED_COLUMNS)
    try:
        data = ws.get_all_records()
        df = pd.DataFrame(data)
        if df.empty:
            return pd.DataFrame(columns=EXPECTED_COLUMNS)

        for col in EXPECTED_COLUMNS:
            if col not in df.columns:
                df[col] = ""

        if "ç´€éŒ„ID" not in df.columns:
            df["ç´€éŒ„ID"] = df.index.astype(str)
        else:
            df["ç´€éŒ„ID"] = df["ç´€éŒ„ID"].astype(str)

        if "ç…§ç‰‡è·¯å¾‘" in df.columns:
            df["ç…§ç‰‡è·¯å¾‘"] = df["ç…§ç‰‡è·¯å¾‘"].fillna("").astype(str)

        numeric_cols = ["å…§æƒåŸå§‹åˆ†", "å¤–æƒåŸå§‹åˆ†", "åƒåœ¾åŸå§‹åˆ†", "æ™¨é–“æ‰“æƒåŸå§‹åˆ†", "æ‰‹æ©Ÿäººæ•¸"]
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)

        if "é€±æ¬¡" in df.columns:
            df["é€±æ¬¡"] = pd.to_numeric(df["é€±æ¬¡"], errors="coerce").fillna(0).astype(int)
    
    except Exception as e:
        st.error(f"è®€å–è³‡æ–™éŒ¯èª¤: {e}")
        return pd.DataFrame(columns=EXPECTED_COLUMNS)
    
    return df[EXPECTED_COLUMNS]

def save_entry(new_entry, uploaded_files=None):
    if "æ—¥æœŸ" in new_entry and new_entry["æ—¥æœŸ"]:
        new_entry["æ—¥æœŸ"] = str(new_entry["æ—¥æœŸ"])

    files_list = [f for f in uploaded_files if f] if uploaded_files else []

    if len(files_list) > 4:
        st.error("âŒ ä¸€æ¬¡æœ€å¤šåªèƒ½ä¸Šå‚³ 4 å¼µç…§ç‰‡ï¼Œè«‹åˆªæ¸›å¾Œå†é€å‡ºã€‚")
        return False

    drive_links = []
    
    # åš´æ ¼æ¨¡å¼ï¼šå‰æ™¯ä¸Šå‚³
    if files_list:
        with st.spinner("â˜ï¸ æ­£åœ¨ä¸Šå‚³ç…§ç‰‡ä¸¦é©—è­‰è­‰æ“š..."):
            links, success = upload_images_parallel(files_list, new_entry)
        
        if not success:
            st.error("ğŸ›‘ **ä¸Šå‚³å¤±æ•—ï¼Œè©•åˆ†æœªé€å‡ºï¼**\n\nç³»çµ±åµæ¸¬åˆ°ç…§ç‰‡ä¸Šå‚³é›²ç«¯å¤±æ•—ï¼Œç‚ºäº†é¿å…ã€Œæœ‰æ‰£åˆ†ç„¡è­‰æ“šã€çš„çˆ­è­°ï¼Œæœ¬ç­†ç´€éŒ„å·²è¢«ç³»çµ±æ””æˆªã€‚\nè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šå¾Œé‡è©¦ã€‚")
            return False
        
        drive_links = links

    if drive_links:
        new_entry["ç…§ç‰‡è·¯å¾‘"] = ";".join(drive_links)

    if "ç´€éŒ„ID" not in new_entry or not new_entry["ç´€éŒ„ID"]:
        unique_suffix = uuid.uuid4().hex[:6]
        timestamp = datetime.now(TW_TZ).strftime("%Y%m%d%H%M%S")
        new_entry["ç´€éŒ„ID"] = f"{timestamp}_{unique_suffix}"

    payload = {"entry": new_entry}

    try:
        res = enqueue_task("main_entry", payload)
        if res == "DB_ERROR":
            st.error("âŒ æœ¬åœ°è³‡æ–™åº«éŒ¯èª¤ï¼Œè«‹é‡æ–°æ•´ç†é é¢ã€‚")
            return False
        return True
    except Exception as e:
        st.error(f"âŒ å¯«å…¥ä½‡åˆ—å¤±æ•—: {e}")
        return False

def save_appeal(entry, proof_file=None):
    image_info = None

    if proof_file:
        try:
            proof_file.seek(0)
            data = proof_file.read()
        except Exception as e:
            st.error(f"âŒ è®€å–ä½è­‰ç…§ç‰‡å¤±æ•—: {e}")
            return False

        if not data:
            st.error("âŒ ä½è­‰ç…§ç‰‡ç‚ºç©ºæª”æ¡ˆ")
            return False
        
        # å­˜åˆ°ç³»çµ±æš«å­˜å€
        logical_fname = f"Appeal_{entry.get('ç­ç´š', '')}_{datetime.now(TW_TZ).strftime('%H%M%S')}.jpg"
        tmp_fname = f"{datetime.now(TW_TZ).strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:6]}_{logical_fname}"
        local_path = os.path.join(IMG_DIR, tmp_fname)
        try:
            with open(local_path, "wb") as f:
                f.write(data)
            image_info = {"path": local_path, "filename": logical_fname}
        except Exception as e:
            st.error(f"âŒ å¯«å…¥ä½è­‰æš«å­˜æª”å¤±æ•—: {e}")
            return False

    if "ç”³è¨´æ—¥æœŸ" not in entry or not entry["ç”³è¨´æ—¥æœŸ"]:
        entry["ç”³è¨´æ—¥æœŸ"] = datetime.now(TW_TZ).strftime("%Y-%m-%d")
    entry["è™•ç†ç‹€æ…‹"] = entry.get("è™•ç†ç‹€æ…‹", "å¾…è™•ç†")
    if "ç™»éŒ„æ™‚é–“" not in entry or not entry["ç™»éŒ„æ™‚é–“"]:
        entry["ç™»éŒ„æ™‚é–“"] = datetime.now(TW_TZ).strftime("%Y-%m-%d %H:%M:%S")
    if "ç”³è¨´ID" not in entry or not entry["ç”³è¨´ID"]:
        entry["ç”³è¨´ID"] = datetime.now(TW_TZ).strftime("%Y%m%d%H%M%S") + "_" + uuid.uuid4().hex[:4]
    if "ä½è­‰ç…§ç‰‡" not in entry:
        entry["ä½è­‰ç…§ç‰‡"] = ""

    payload = {
        "entry": entry,
        "image_file": image_info,
    }
    enqueue_task("appeal_entry", payload)
    st.success("ğŸ“© ç”³è¨´å·²æ’å…¥èƒŒæ™¯è™•ç†")
    return True

@st.cache_data(ttl=60)
def load_appeals():
    ws = get_worksheet(SHEET_TABS["appeals"])
    if not ws:
        return pd.DataFrame(columns=APPEAL_COLUMNS)

    try:
        records = ws.get_all_records()
        df = pd.DataFrame(records)
    except Exception:
        return pd.DataFrame(columns=APPEAL_COLUMNS)

    for col in APPEAL_COLUMNS:
        if col not in df.columns:
            if col == "è™•ç†ç‹€æ…‹":
                df[col] = "å¾…è™•ç†"
            else:
                df[col] = ""

    df = df[APPEAL_COLUMNS]
    return df

def delete_rows_by_ids(record_ids_to_delete):
    ws = get_worksheet(SHEET_TABS["main"])
    if not ws: return False
    try:
        records = ws.get_all_records()
        rows_to_delete = []
        for i, record in enumerate(records):
            if str(record.get("ç´€éŒ„ID")) in record_ids_to_delete:
                rows_to_delete.append(i + 2)
        
        rows_to_delete.sort(reverse=True)
        for row_idx in rows_to_delete:
            ws.delete_rows(row_idx)
            time.sleep(0.8)
    
        st.cache_data.clear()
        return True
    except Exception as e:
        st.error(f"åˆªé™¤å¤±æ•—: {e}")
        return False

def update_appeal_status(appeal_row_idx, status, record_id):
    ws_appeals = get_worksheet(SHEET_TABS["appeals"])
    ws_main = get_worksheet(SHEET_TABS["main"])
    try:
        appeals_data = ws_appeals.get_all_records()
        target_row = None
        for i, row in enumerate(appeals_data):
            if str(row.get("å°æ‡‰ç´€éŒ„ID")) == str(record_id) and str(row.get("è™•ç†ç‹€æ…‹")) == "å¾…è™•ç†":
                target_row = i + 2 
                break
        if target_row:
            col_idx = APPEAL_COLUMNS.index("è™•ç†ç‹€æ…‹") + 1
            ws_appeals.update_cell(target_row, col_idx, status)
            if status == "å·²æ ¸å¯" and record_id:
                main_data = ws_main.get_all_records()
                main_target_row = None
                for j, m_row in enumerate(main_data):
                    if str(m_row.get("ç´€éŒ„ID")) == str(record_id):
                        main_target_row = j + 2
                        break
                if main_target_row:
                    fix_col_idx = EXPECTED_COLUMNS.index("ä¿®æ­£") + 1
                    ws_main.update_cell(main_target_row, fix_col_idx, "TRUE")
            st.cache_data.clear()
            return True, "æ›´æ–°æˆåŠŸ"
        else: return False, "æ‰¾ä¸åˆ°å°æ‡‰çš„ç”³è¨´åˆ—"
    except Exception as e: return False, str(e)

@st.cache_data(ttl=21600)
def load_roster_dict():
    ws = get_worksheet(SHEET_TABS["roster"])
    roster_dict = {}
    if ws:
        try:
            df = pd.DataFrame(ws.get_all_records())
            id_col = next((c for c in df.columns if "å­¸è™Ÿ" in c), None)
            class_col = next((c for c in df.columns if "ç­ç´š" in c), None)
            if id_col and class_col:
                for _, row in df.iterrows():
                    sid = clean_id(row[id_col])
                    if sid: roster_dict[sid] = str(row[class_col]).strip()
        except: pass
    return roster_dict
    
@st.cache_data(ttl=3600)
def load_sorted_classes():
    ws = get_worksheet(SHEET_TABS["roster"])
    if not ws: return [], []
    try:
        df = pd.DataFrame(ws.get_all_records())
        class_col = next((c for c in df.columns if "ç­ç´š" in c), None)
        if not class_col: return [], []
        
        unique_classes = df[class_col].dropna().unique().tolist()
        unique_classes = [str(c).strip() for c in unique_classes if str(c).strip()]
        
        dept_order = {"å•†": 1, "è‹±": 2, "è³‡": 3, "å®¶": 4, "æœ": 5}
        
        def get_sort_key(name):
            grade = 99
            if "ä¸€" in name or "1" in name: grade = 1
            if "äºŒ" in name or "2" in name: grade = 2
            if "ä¸‰" in name or "3" in name: grade = 3
            
            dept_score = 99
            for k, v in dept_order.items():
                if k in name:
                    dept_score = v
                    break
            return (grade, dept_score, name)
        
        sorted_all = sorted(unique_classes, key=get_sort_key)
        
        structured = []
        for c in sorted_all:
            grade_val = get_sort_key(c)[0]
            g_label = f"{grade_val}å¹´ç´š" if grade_val != 99 else "å…¶ä»–"
            structured.append({"grade": g_label, "name": c})
            
        return sorted_all, structured
    except Exception as e:
        print(f"Sorting Error: {e}")
        return [], []

@st.cache_data(ttl=21600)
def load_teacher_emails():
    ws = get_worksheet(SHEET_TABS["teachers"])
    email_dict = {}
    if ws:
        try:
            df = pd.DataFrame(ws.get_all_records())
            class_col = next((c for c in df.columns if "ç­ç´š" in c), None)
            mail_col = next((c for c in df.columns if "Email" in c or "ä¿¡ç®±" in c or "éƒµä»¶" in c), None)
            name_col = next((c for c in df.columns if "å°å¸«" in c or "å§“å" in c), None)
            if class_col and mail_col:
                for _, row in df.iterrows():
                    cls = str(row[class_col]).strip()
                    mail = str(row[mail_col]).strip()
                    name = str(row[name_col]).strip() if name_col else "è€å¸«"
                    if cls and mail and "@" in mail:
                        email_dict[cls] = {"email": mail, "name": name}
        except: pass
    return email_dict

@st.cache_data(ttl=21600)
def load_inspector_list():
    ws = get_worksheet(SHEET_TABS["inspectors"])
    default = [{"label": "æ¸¬è©¦äººå“¡", "allowed_roles": ["å…§æƒæª¢æŸ¥"], "assigned_classes": [], "id_prefix": "æ¸¬"}]
    if not ws: return default
    try:
        df = pd.DataFrame(ws.get_all_records())
        if df.empty: return default
        inspectors = []
        id_col = next((c for c in df.columns if "å­¸è™Ÿ" in c or "ç·¨è™Ÿ" in c), None)
        role_col = next((c for c in df.columns if "è² è²¬" in c or "é …ç›®" in c), None)
        scope_col = next((c for c in df.columns if "ç­ç´š" in c or "ç¯„åœ" in c), None)
        if id_col:
            for _, row in df.iterrows():
                s_id = clean_id(row[id_col])
                s_role = str(row[role_col]).strip() if role_col else ""
                allowed = []
                if "çµ„é•·" in s_role: allowed = ["å…§æƒæª¢æŸ¥", "å¤–æƒæª¢æŸ¥", "åƒåœ¾/å›æ”¶æª¢æŸ¥", "æ™¨é–“æ‰“æƒ"]
                elif "æ©Ÿå‹•" in s_role: allowed = ["å…§æƒæª¢æŸ¥", "å¤–æƒæª¢æŸ¥", "åƒåœ¾/å›æ”¶æª¢æŸ¥"]
                else:
                    if "å¤–æƒ" in s_role: allowed.append("å¤–æƒæª¢æŸ¥")
                    if "åƒåœ¾" in s_role: allowed.append("åƒåœ¾/å›æ”¶æª¢æŸ¥")
                    if "æ™¨" in s_role: allowed.append("æ™¨é–“æ‰“æƒ")
                    if "å…§æƒ" in s_role: allowed.append("å…§æƒæª¢æŸ¥")
                if not allowed: allowed = ["å…§æƒæª¢æŸ¥"]
            
                s_classes = []
                if scope_col and str(row[scope_col]):
                    raw = str(row[scope_col])
                    s_classes = [c.strip() for c in raw.replace("ã€", ";").replace(",", ";").split(";") if c.strip()]
                prefix = s_id[0] if len(s_id) > 0 else "X"
                inspectors.append({"label": f"å­¸è™Ÿ: {s_id}", "allowed_roles": allowed, "assigned_classes": s_classes, "id_prefix": prefix})
        return inspectors if inspectors else default
    except: return default

@st.cache_data(ttl=60)
def get_daily_duty(target_date):
    ws = get_worksheet(SHEET_TABS["duty"])
    if not ws: return [], "error"
    try:
        df = pd.DataFrame(ws.get_all_records())
        if df.empty: return [], "no_data"
        date_col = next((c for c in df.columns if "æ—¥æœŸ" in c), None)
        id_col = next((c for c in df.columns if "å­¸è™Ÿ" in c), None)
        loc_col = next((c for c in df.columns if "åœ°é»" in c), None)
        if date_col and id_col:
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
            t_date = target_date if isinstance(target_date, date) else target_date.date()
            today_df = df[df[date_col] == t_date]
            res = []
            for _, row in today_df.iterrows():
                res.append({"å­¸è™Ÿ": clean_id(row[id_col]), "æƒåœ°å€åŸŸ": str(row[loc_col]).strip() if loc_col else "", "å·²å®Œæˆæ‰“æƒ": False})
            return res, "success"
        return [], "missing_cols"
    except: return [], "error"

@st.cache_data(ttl=21600)
def load_settings():
    ws = get_worksheet(SHEET_TABS["settings"])
    config = {"semester_start": "2025-08-25"}
    if ws:
        try:
            data = ws.get_all_values()
            for row in data:
                if len(row)>=2 and row[0] == "semester_start": config["semester_start"] = row[1]
        except: pass
    return config

def save_setting(key, val):
    ws = get_worksheet(SHEET_TABS["settings"])
    if ws:
        try:
            cell = ws.find(key)
            if cell: ws.update_cell(cell.row, cell.col+1, val)
            else: ws.append_row([key, val])
            st.cache_data.clear()
            return True
        except: return False
    return False

def send_bulk_emails(email_list):
    if "system_config" not in st.secrets:
        return 0, "Secrets æœªè¨­å®š"
    
    sender_email = st.secrets["system_config"].get("smtp_email")
    sender_password = st.secrets["system_config"].get("smtp_password")
    if not sender_email or not sender_password: return 0, "Secrets æœªè¨­å®š Email"

    sent_count = 0
    try:
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(sender_email, sender_password)
        for item in email_list:
            try:
                msg = MIMEMultipart()
                msg['From'] = sender_email
                msg['To'] = item['email']
                msg['Subject'] = item['subject']
                msg.attach(MIMEText(item['body'], 'plain'))
                server.sendmail(sender_email, item['email'], msg.as_string())
                sent_count += 1
            except Exception as inner_e:
                print(f"å€‹åˆ¥å¯„é€å¤±æ•—: {inner_e}")
        server.quit()
        return sent_count, "ç™¼é€ä½œæ¥­çµæŸ"
    except Exception as e:
        return sent_count, str(e)

def check_duplicate_record(df, check_date, inspector, role, target_class=None):
    if df.empty: return False
    try:
        df["æ—¥æœŸStr"] = df["æ—¥æœŸ"].astype(str)
        check_date_str = str(check_date)
        mask = (df["æ—¥æœŸStr"] == check_date_str) & (df["æª¢æŸ¥äººå“¡"] == inspector) & (df["è©•åˆ†é …ç›®"] == role)
        if target_class: mask = mask & (df["ç­ç´š"] == target_class)
        return not df[mask].empty
    except: return False

# ==========================================
# 5. ä¸»ç¨‹å¼ä»‹é¢
# ==========================================

# è®€å–è¨­å®šèˆ‡è³‡æ–™
SYSTEM_CONFIG = load_settings()
ROSTER_DICT = load_roster_dict()
INSPECTOR_LIST = load_inspector_list()
TEACHER_MAILS = load_teacher_emails()

all_classes, structured_classes = load_sorted_classes()
if not all_classes:
    all_classes = ["æ¸¬è©¦ç­ç´š"]
    structured_classes = [{"grade": "å…¶ä»–", "name": "æ¸¬è©¦ç­ç´š"}]

grades = sorted(list(set([c["grade"] for c in structured_classes])))

def get_week_num(d):
    try:
        start = datetime.strptime(SYSTEM_CONFIG["semester_start"], "%Y-%m-%d").date()
        if isinstance(d, datetime): d = d.date()
        return max(0, ((d - start).days // 7) + 1)
    except: return 0

now_tw = datetime.now(TW_TZ)
today_tw = now_tw.date()

st.sidebar.title("ğŸ« åŠŸèƒ½é¸å–®")
app_mode = st.sidebar.radio("è«‹é¸æ“‡æ¨¡å¼", ["ç³¾å¯Ÿåº•å®¶ğŸ‘€", "ç­ç´šè² è²¬äººğŸ¥¸", "çµ„é•·ã„‰çª©ğŸ’ƒ"])

with st.sidebar.expander("ğŸ”§ ç³»çµ±é€£ç·šè¨ºæ–·", expanded=False):
    if get_gspread_client(): 
        st.success("âœ… Google Sheets é€£ç·šæ­£å¸¸")
    else: 
        st.error("âŒ Sheets é€£ç·šå¤±æ•—")
        
    if "gcp_service_account" in st.secrets: 
        st.success("âœ… GCP æ†‘è­‰å·²è®€å–")
    else: 
        st.error("âš ï¸ æœªè¨­å®š GCP Service Account")
        
    if "system_config" in st.secrets and "drive_folder_id" in st.secrets["system_config"]:
        st.success("âœ… Drive è³‡æ–™å¤¾ ID å·²è¨­å®š")
    else:
        st.warning("âš ï¸ æœªè¨­å®š Drive è³‡æ–™å¤¾ ID")

# --- æ¨¡å¼1: ç³¾å¯Ÿè©•åˆ† ---
if app_mode == "ç³¾å¯Ÿåº•å®¶ğŸ‘€":
    st.title("ğŸ“ è¡›ç”Ÿç³¾å¯Ÿè©•åˆ†ç³»çµ±")
    if "team_logged_in" not in st.session_state: st.session_state["team_logged_in"] = False
    
    if not st.session_state["team_logged_in"]:
        with st.expander("ğŸ” èº«ä»½é©—è­‰", expanded=True):
            input_code = st.text_input("è«‹è¼¸å…¥éšŠä¼é€šè¡Œç¢¼", type="password")
            if st.button("ç™»å…¥"):
                if input_code == st.secrets["system_config"]["team_password"]:
                    st.session_state["team_logged_in"] = True
                    st.rerun()
                else: st.error("é€šè¡Œç¢¼éŒ¯èª¤")
    
    if st.session_state["team_logged_in"]:
        prefixes = sorted(list(set([p["id_prefix"] for p in INSPECTOR_LIST])))
        prefix_labels = [f"{p}é–‹é ­" for p in prefixes]
        if not prefix_labels: st.warning("æ‰¾ä¸åˆ°ç³¾å¯Ÿåå–®ï¼Œè«‹é€šçŸ¥è€å¸«åœ¨å¾Œå°å»ºç«‹åå–® (Sheet: inspectors)ã€‚")
        else:
            selected_prefix_label = st.radio("æ­¥é©Ÿ 1ï¼šé¸æ“‡é–‹é ­", prefix_labels, horizontal=True)
            selected_prefix = selected_prefix_label[0]
            filtered_inspectors = [p for p in INSPECTOR_LIST if p["id_prefix"] == selected_prefix]
            inspector_name = st.radio("æ­¥é©Ÿ 2ï¼šé»é¸èº«ä»½", [p["label"] for p in filtered_inspectors])
            current_inspector_data = next((p for p in INSPECTOR_LIST if p["label"] == inspector_name), None)
            allowed_roles = current_inspector_data.get("allowed_roles", ["å…§æƒæª¢æŸ¥"])
            
            allowed_roles = [r for r in allowed_roles if r != "æ™¨é–“æ‰“æƒ"]
            if not allowed_roles: allowed_roles = ["å…§æƒæª¢æŸ¥"] 
            assigned_classes = current_inspector_data.get("assigned_classes", [])
            
            st.markdown("---")
            col_date, col_role = st.columns(2)
            input_date = col_date.date_input("æª¢æŸ¥æ—¥æœŸ", today_tw)
            if len(allowed_roles) > 1: role = col_role.radio("è«‹é¸æ“‡æª¢æŸ¥é …ç›®", allowed_roles, horizontal=True)
            else: role = allowed_roles[0]
            col_role.info(f"ğŸ“‹ æ‚¨çš„è² è²¬é …ç›®ï¼š**{role}**")
            
            week_num = get_week_num(input_date)
            st.caption(f"ğŸ“… ç¬¬ {week_num} é€±")
            main_df = load_main_data()

            if role == "åƒåœ¾/å›æ”¶æª¢æŸ¥":
                st.info("ğŸ—‘ï¸ å…¨æ ¡åƒåœ¾æª¢æŸ¥ (æ¯æ—¥æ¯ç­ä¸Šé™æ‰£2åˆ†)")
                trash_cat = st.radio("é•è¦é …ç›®", ["ä¸€èˆ¬åƒåœ¾", "ç´™é¡", "ç¶²è¢‹", "å…¶ä»–å›æ”¶"], horizontal=True)
                with st.form("trash_form"):
                    t_data = [{"ç­ç´š": c, "ç„¡ç°½å": False, "ç„¡åˆ†é¡": False} for c in all_classes]
                    edited_t_df = st.data_editor(pd.DataFrame(t_data), hide_index=True, height=400, use_container_width=True)
                    if st.form_submit_button("é€å‡º"):
                        base = {"æ—¥æœŸ": input_date, "é€±æ¬¡": week_num, "æª¢æŸ¥äººå“¡": inspector_name, "ç™»éŒ„æ™‚é–“": now_tw.strftime("%Y-%m-%d %H:%M:%S"), "ä¿®æ­£": False}
                        cnt = 0
                        for _, row in edited_t_df.iterrows():
                            vios = []
                            if row["ç„¡ç°½å"]: vios.append("ç„¡ç°½å")
                            if row["ç„¡åˆ†é¡"]: vios.append("ç„¡åˆ†é¡")
                            if vios:
                                save_entry({**base, "ç­ç´š": row["ç­ç´š"], "è©•åˆ†é …ç›®": role, "åƒåœ¾åŸå§‹åˆ†": len(vios), "å‚™è¨»": f"{trash_cat}-{'ã€'.join(vios)}", "é•è¦ç´°é …": trash_cat})
                                cnt += 1
                        st.success(f"å·²æ’å…¥èƒŒæ™¯è™•ç†ï¼š {cnt} ç­" if cnt else "ç„¡é•è¦"); st.rerun()
            else:
                st.markdown("### ğŸ« é¸æ“‡å—æª¢ç­ç´š")
                if assigned_classes:
                    radio_key = f"radio_assigned_{inspector_name}"
                    selected_class = st.radio(
                        "è«‹é»é¸æ‚¨çš„è² è²¬ç­ç´š", 
                        assigned_classes, 
                        key=radio_key
                    )
                else:
                    g = st.radio("æ­¥é©Ÿ A: é¸æ“‡å¹´ç´š", grades, horizontal=True, key="radio_grade_select")
                    filtered_classes = [c["name"] for c in structured_classes if c["grade"] == g]
                    
                    if not filtered_classes:
                        st.warning("âš ï¸ æ­¤å¹´ç´šç„¡ç­ç´šè³‡æ–™")
                        selected_class = None
                    else:
                        selected_class = st.radio(
                            "æ­¥é©Ÿ B: é¸æ“‡ç­ç´š", 
                            filtered_classes, 
                            horizontal=True,
                            key=f"radio_class_select_{g}"
                        )
        
                if selected_class:
                    st.markdown(f"ğŸ‘‰ ç›®å‰é–å®šè©•åˆ†å°è±¡ï¼š **<span style='color:red;font-size:1.2em'>{selected_class}</span>**", unsafe_allow_html=True)
                    if check_duplicate_record(main_df, input_date, inspector_name, role, selected_class):
                            st.warning(f"âš ï¸ æ³¨æ„ï¼šæ‚¨ä»Šå¤©å·²ç¶“è©•éã€Œ{selected_class}ã€äº†ï¼")
                    st.info(f"ğŸ“ æ­£åœ¨è©•åˆ†ï¼š**{selected_class}**")
                    
                    day_key = f"{str(input_date)}|{inspector_name}|{role}"
                    if "scored_map" not in st.session_state:
                        st.session_state["scored_map"] = {}
                    scored_today = st.session_state["scored_map"].setdefault(day_key, set())

                    if selected_class in scored_today:
                        st.success(f"âœ… ä»Šæ—¥ã€Œ{selected_class}ã€å·²è©•åˆ†ï¼ˆæœ¬æ¬¡ç™»å…¥æœŸé–“ï¼‰")

                    form_id = f"scoring_form_{str(input_date)}_{inspector_name}_{role}_{selected_class}"

                    with st.form(form_id, clear_on_submit=True):
                        in_s = 0
                        out_s = 0
                        ph_c = 0
                        note = ""

                        result_key = f"result_{form_id}"
                        note_key = f"note_{form_id}"
                        phone_key = f"phone_{form_id}"
                        in_key = f"in_{form_id}"
                        out_key = f"out_{form_id}"
                        fix_key = f"fix_{form_id}"
                        files_key = f"files_{form_id}"

                        if role == "å…§æƒæª¢æŸ¥":
                            result = st.radio("çµæœ", ["âŒ é•è¦", "âœ¨ ä¹¾æ·¨"], horizontal=True, key=result_key)
                            if result == "âŒ é•è¦":
                                in_s = st.number_input("å…§æƒæ‰£åˆ† (ä¸Šé™2åˆ†)", min_value=0, max_value=2, value=0, step=1, key=in_key)
                                note = st.text_input("èªªæ˜", placeholder="é»‘æ¿æœªæ“¦", key=note_key)
                                ph_c = st.number_input("æ‰‹æ©Ÿäººæ•¸ (ç„¡ä¸Šé™)", min_value=0, value=0, step=1, key=phone_key)
                            else:
                                note = "ã€å„ªè‰¯ã€‘"

                        elif role == "å¤–æƒæª¢æŸ¥":
                            result = st.radio("çµæœ", ["âŒ é•è¦", "âœ¨ ä¹¾æ·¨"], horizontal=True, key=result_key)
                            if result == "âŒ é•è¦":
                                out_s = st.number_input("å¤–æƒæ‰£åˆ† (ä¸Šé™2åˆ†)", min_value=0, max_value=2, value=0, step=1, key=out_key)
                                note = st.text_input("èªªæ˜", placeholder="èµ°å»Šåƒåœ¾", key=note_key)
                                ph_c = st.number_input("æ‰‹æ©Ÿäººæ•¸ (ç„¡ä¸Šé™)", min_value=0, value=0, step=1, key=phone_key)
                            else:
                                note = "ã€å„ªè‰¯ã€‘"

                        is_fix = st.checkbox("ğŸš© ä¿®æ­£å–®", key=fix_key)
                        files = st.file_uploader("ç…§ç‰‡(è‡ªå‹•ä¸Šå‚³é›²ç«¯)", accept_multiple_files=True, type=["jpg", "jpeg", "png"], key=files_key)

                        if st.form_submit_button("é€å‡º"):
                            if files and len(files) > 4:
                                st.error("âŒ ä¸€æ¬¡æœ€å¤šåªèƒ½ä¸Šå‚³ 4 å¼µç…§ç‰‡ï¼Œè«‹åˆªæ¸›å¾Œå†é€å‡ºã€‚")
                            else:
                                ok = save_entry(
                                    {"æ—¥æœŸ": input_date, "é€±æ¬¡": week_num, "æª¢æŸ¥äººå“¡": inspector_name, "ç™»éŒ„æ™‚é–“": now_tw.strftime("%Y-%m-%d %H:%M:%S"),
                                        "ä¿®æ­£": is_fix, "ç­ç´š": selected_class, "è©•åˆ†é …ç›®": role, "å…§æƒåŸå§‹åˆ†": in_s, "å¤–æƒåŸå§‹åˆ†": out_s, "æ‰‹æ©Ÿäººæ•¸": ph_c, "å‚™è¨»": note},
                                    uploaded_files=files
                                )
                                if ok:
                                    scored_today.add(selected_class)
                                    st.toast(f"âœ… å·²æˆåŠŸé€å‡ºï¼š{selected_class}ï¼ˆç…§ç‰‡å·²ä¸Šå‚³é›²ç«¯ï¼‰")
                                    time.sleep(1) # è®“ toast é¡¯ç¤º
                                    st.rerun()

# --- æ¨¡å¼2: è¡›ç”Ÿè‚¡é•· ---
elif app_mode == "ç­ç´šè² è²¬äººğŸ¥¸":
    st.title("ğŸ” ç­ç´šæˆç¸¾æŸ¥è©¢")
    df = load_main_data()
    
    appeals_df = load_appeals()
    appeal_map = {}
    if not appeals_df.empty:
        for _, a_row in appeals_df.iterrows():
            rid = str(a_row.get("å°æ‡‰ç´€éŒ„ID", "")).strip()
            if rid:
                appeal_map[rid] = a_row.get("è™•ç†ç‹€æ…‹", "å¾…è™•ç†")

    if not df.empty:
        st.write("è«‹ä¾ç…§æ­¥é©Ÿé¸æ“‡ï¼š")
        g = st.radio("æ­¥é©Ÿ 1ï¼šé¸æ“‡å¹´ç´š", grades, horizontal=True)
        class_options = [c["name"] for c in structured_classes if c["grade"] == g]
        
        if not class_options:
            st.warning("æŸ¥ç„¡æ­¤å¹´ç´šç­ç´š")
            cls = None
        else:
            cls = st.radio("æ­¥é©Ÿ 2ï¼šé¸æ“‡ç­ç´š", class_options, horizontal=True)

        st.divider()
        
        if cls:
            c_df = df[df["ç­ç´š"] == cls].sort_values("ç™»éŒ„æ™‚é–“", ascending=False)
            three_days_ago = date.today() - timedelta(days=3)
            
            if not c_df.empty:
                st.subheader(f"ğŸ“Š {cls} è¿‘æœŸç´€éŒ„èˆ‡ç”³è¨´ç‹€æ…‹")
        
                for idx, r in c_df.iterrows():
                    total_raw = r['å…§æƒåŸå§‹åˆ†']+r['å¤–æƒåŸå§‹åˆ†']+r['åƒåœ¾åŸå§‹åˆ†']+r['æ™¨é–“æ‰“æƒåŸå§‹åˆ†']
                    phone_msg = f" | ğŸ“±æ‰‹æ©Ÿ: {r['æ‰‹æ©Ÿäººæ•¸']}" if r['æ‰‹æ©Ÿäººæ•¸'] > 0 else ""
                    
                    record_id = str(r['ç´€éŒ„ID']).strip()
                    appeal_status = appeal_map.get(record_id, None)
            
                    status_icon = ""
                    if appeal_status == "å·²æ ¸å¯": status_icon = "âœ… [ç”³è¨´æˆåŠŸ] "
                    elif appeal_status == "å·²é§å›": status_icon = "ğŸš« [ç”³è¨´é§å›] "
                    elif appeal_status == "å¾…è™•ç†": status_icon = "â³ [å¯©æ ¸ä¸­] "
                    elif str(r['ä¿®æ­£']) == "TRUE": status_icon = "ğŸ› ï¸ [å·²ä¿®æ­£] "

                    week_val = r.get('é€±æ¬¡', 0)
                    week_label = f"[ç¬¬{week_val}é€±] " if week_val and str(week_val) != "0" else ""

                    title_text = f"{status_icon}{week_label}{r['æ—¥æœŸ']} - {r['è©•åˆ†é …ç›®']} (æ‰£åˆ†: {total_raw}){phone_msg}"
                    
                    with st.expander(title_text):
                        if appeal_status == "å·²æ ¸å¯":
                            st.success("ğŸ‰ æ­å–œï¼è¡›ç”Ÿçµ„å·²æ ¸å¯æ‚¨çš„ç”³è¨´ï¼Œæœ¬ç­†æ‰£åˆ†å·²æ’¤éŠ·ã€‚")
                        elif appeal_status == "å·²é§å›":
                            st.error("ğŸ›‘ å¾ˆéºæ†¾ï¼Œæ‚¨çš„ç”³è¨´å·²è¢«é§å›ï¼Œç¶­æŒåŸåˆ¤ã€‚")
                        elif appeal_status == "å¾…è™•ç†":
                            st.warning("â³ ç”³è¨´æ¡ˆä»¶ç›®å‰æ­£åœ¨æ’éšŠå¯©æ ¸ä¸­ï¼Œè«‹è€å¿ƒç­‰å€™...")

                        st.write(f"ğŸ“ èªªæ˜: {r['å‚™è¨»']}")
                        st.caption(f"æª¢æŸ¥äººå“¡: {r['æª¢æŸ¥äººå“¡']}")
                        
                        raw_photo_path = str(r.get("ç…§ç‰‡è·¯å¾‘", "")).strip()
                        if raw_photo_path and raw_photo_path.lower() != "nan":
                            path_list = [p.strip() for p in raw_photo_path.split(";") if p.strip()]
                            valid_photos = [p for p in path_list if p != "UPLOAD_FAILED" and (p.startswith("http") or os.path.exists(p))]
                            if valid_photos:
                                captions = [f"é•è¦ç…§ç‰‡ ({i+1})" for i in range(len(valid_photos))]
                                st.image(valid_photos, caption=captions, width=300)
                            elif "UPLOAD_FAILED" in path_list: st.warning("âš ï¸ ç…§ç‰‡ä¸Šå‚³å¤±æ•—")

                        if total_raw > 2 and r['æ™¨é–“æ‰“æƒåŸå§‹åˆ†'] == 0:
                            st.info("ğŸ’¡ç³»çµ±æç¤ºï¼šå–®é …æ¯æ—¥æ‰£åˆ†ä¸Šé™ç‚º 2 åˆ† (æ‰‹æ©Ÿã€æ™¨æƒé™¤å¤–)ï¼Œæœ€çµ‚æˆç¸¾å°‡ç”±å¾Œå°è‡ªå‹•è¨ˆç®—ä¸Šé™ã€‚")

                        record_date_obj = pd.to_datetime(r['æ—¥æœŸ']).date() if isinstance(r['æ—¥æœŸ'], str) else r['æ—¥æœŸ']
        
                        if appeal_status:
                            pass 
                        elif record_date_obj >= three_days_ago and (total_raw > 0 or r['æ‰‹æ©Ÿäººæ•¸'] > 0):
                            st.markdown("---")
                            st.markdown("#### ğŸš¨ æˆ‘è¦ç”³è¨´")
                            form_key = f"appeal_form_{r['ç´€éŒ„ID']}_{idx}"
                            with st.form(form_key):
                                reason = st.text_area("ç”³è¨´ç†ç”±", height=80, placeholder="è©³ç´°èªªæ˜...")
                                proof_file = st.file_uploader("ä¸Šå‚³ä½è­‰ (å¿…å¡«)", type=["jpg", "png", "jpeg"], key=f"file_{idx}")
                                if st.form_submit_button("æäº¤ç”³è¨´"):
                                    if not reason or not proof_file: 
                                        st.error("âŒ è«‹å¡«å¯«ç†ç”±ä¸¦ä¸Šå‚³ç…§ç‰‡")
                                    else:
                                        appeal_entry = {
                                            "ç”³è¨´æ—¥æœŸ": str(date.today()), 
                                            "ç­ç´š": cls, 
                                            "é•è¦æ—¥æœŸ": str(r["æ—¥æœŸ"]),
                                            "é•è¦é …ç›®": f"{r['è©•åˆ†é …ç›®']} ({r['å‚™è¨»']})", 
                                            "åŸå§‹æ‰£åˆ†": str(total_raw),
                                            "ç”³è¨´ç†ç”±": reason, 
                                            "è™•ç†ç‹€æ…‹": "å¾…è™•ç†",
                                            "ç™»éŒ„æ™‚é–“": datetime.now(TW_TZ).strftime("%Y-%m-%d %H:%M:%S"),
                                            "å°æ‡‰ç´€éŒ„ID": r['ç´€éŒ„ID']
                                        }
                                        if save_appeal(appeal_entry, proof_file): 
                                            st.success("âœ… ç”³è¨´å·²æäº¤ï¼è«‹é‡æ–°æ•´ç†é é¢æŸ¥çœ‹ç‹€æ…‹ã€‚")
                                            time.sleep(1.5)
                                            st.rerun()
                                        else: 
                                            st.error("æäº¤å¤±æ•—")
                        elif total_raw > 0 and not appeal_status:
                            st.caption("â³ Sorry Broï¼Œå·²è¶…é 3 å¤©ç”³è¨´æœŸé™ã€‚")
            else:
                st.info("ğŸ‰ æœ€è¿‘æ²’æœ‰é•è¦ç´€éŒ„ï¼Œå°¼å€‘ç­å¾ˆè®šï¼")

# --- æ¨¡å¼3: å¾Œå° ---
elif app_mode == "çµ„é•·ã„‰çª©ğŸ’ƒ":
    st.title("âš™ï¸ ç®¡ç†å¾Œå°")
    
    # ç¢ºä¿ Worker å•Ÿå‹•ä»¥æ›´æ–°ç‹€æ…‹
    ensure_worker_started()
    
    # [SRE] ç›£æ§é¢æ¿
    metrics = get_queue_metrics()
    q_count = metrics["pending"] + metrics["retry"]
    oldest_age = metrics["oldest_pending_sec"]
    recent_errs = metrics["recent_errors"]
    
    with st.container(border=True):
        st.write("#### ğŸ“¡ SRE ç›£æ§é¢æ¿")
        m1, m2, m3, m4 = st.columns(4)
        m1.metric("Pending Queue", q_count, delta="Safe" if q_count < 50 else "High Load", delta_color="inverse")
        m2.metric("Running Tasks", metrics["running"])
        m3.metric("Failed Tasks", metrics["failed"])
        m4.metric("Oldest Task Age", f"{int(oldest_age)}s", delta="Lagging" if oldest_age > 300 else "Normal", delta_color="inverse")

        if q_count > 100:
            st.error(f"ğŸ”¥ **ç³»çµ±éè¼‰è­¦å‘Š**ï¼šç©å£“ {q_count} ç­†è³‡æ–™ï¼")
        elif oldest_age > 300:
            st.warning(f"ğŸ¢ **å¯«å…¥å»¶é²è­¦å‘Š**ï¼šæ»¯ç•™ {int(oldest_age)} ç§’ã€‚")
        
        if recent_errs:
            with st.expander("æŸ¥çœ‹æœ€è¿‘éŒ¯èª¤æ—¥èªŒ (Top 5)"):
                for err_msg, ts in recent_errs:
                    st.error(f"[{ts}] {err_msg}")

    pwd = st.text_input("ç®¡ç†å¯†ç¢¼", type="password")
    if pwd == st.secrets["system_config"]["admin_password"]:
        monitor_tab, tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
            "ğŸ‘€ é€²åº¦ç›£æ§", "ğŸ“Š æˆç¸¾ç¸½è¡¨", "ğŸ“ æ‰£åˆ†æ˜ç´°", "ğŸ“§ å¯„é€é€šçŸ¥", 
            "ğŸ“£ ç”³è¨´å¯©æ ¸", "âš™ï¸ ç³»çµ±è¨­å®š", "ğŸ“„ åå–®æ›´æ–°", "ğŸ§¹ æ™¨æƒé»å"
        ])
        
        with monitor_tab:
            st.subheader("ğŸ•µï¸ ä»Šæ—¥è©•åˆ†é€²åº¦ç›£æ§")
            
            # 1. è¨­å®šç›£æ§æ—¥æœŸ (é è¨­ä»Šå¤©)
            monitor_date = st.date_input("ç›£æ§æ—¥æœŸ", today_tw, key="monitor_date")
            st.caption(f"ğŸ“… æª¢æŸ¥ç›®æ¨™ï¼š{monitor_date} (å»ºè­°æ–¼ 16:30 å‰å®Œæˆ)")

            # 2. æº–å‚™è³‡æ–™
            df = load_main_data()
            
            # å–å¾—ä»Šæ—¥å·²å›å ±çš„äººå“¡åå–® (å»é‡)
            submitted_names = set()
            if not df.empty:
                # è½‰æˆå­—ä¸²æ¯”å°ï¼Œç¢ºä¿æ ¼å¼ä¸€è‡´
                df["æ—¥æœŸStr"] = df["æ—¥æœŸ"].astype(str)
                target_str = str(monitor_date)
                today_records = df[df["æ—¥æœŸStr"] == target_str]
                submitted_names = set(today_records["æª¢æŸ¥äººå“¡"].unique())

            # 3. åˆ†é¡æª¢æŸ¥äººå“¡ (ä¸€èˆ¬è©•åˆ† vs æ©Ÿå‹•/çµ„é•·)
            # é‚è¼¯ï¼šæœ‰åˆ†é… "assigned_classes" çš„æ˜¯ç­ç´šè©•åˆ†å“¡ï¼Œæ²’æœ‰çš„æ˜¯æ©Ÿå‹•
            regular_inspectors = [] # æœ‰å›ºå®šç­ç´š
            mobile_inspectors = []  # æ©Ÿå‹•/çµ„é•· (ç„¡å›ºå®šç­ç´š)

            for p in INSPECTOR_LIST:
                p_name = p["label"]
                # åˆ¤æ–·æ˜¯å¦ç‚ºæ©Ÿå‹•ï¼šçœ‹ assigned_classes æ˜¯å¦ç‚ºç©º
                is_mobile = len(p.get("assigned_classes", [])) == 0
                
                # å»ºç«‹ç‹€æ…‹ç‰©ä»¶
                status_obj = {
                    "name": p_name,
                    "role_desc": "ã€".join(p.get("allowed_roles", [])),
                    "done": p_name in submitted_names
                }

                if is_mobile:
                    mobile_inspectors.append(status_obj)
                else:
                    regular_inspectors.append(status_obj)

            # 4. é¡¯ç¤ºå„€è¡¨æ¿
            # --- è¨ˆç®—æ•¸æ“š ---
            total_regular = len(regular_inspectors)
            done_regular = sum(1 for x in regular_inspectors if x["done"])
            
            total_mobile = len(mobile_inspectors)
            done_mobile = sum(1 for x in mobile_inspectors if x["done"])

            # --- é¡¯ç¤ºé€²åº¦æ¢ ---
            c1, c2 = st.columns(2)
            with c1:
                st.metric("ç­ç´šè©•åˆ†å“¡å®Œæˆç‡", f"{done_regular}/{total_regular}", delta=f"å°šç¼º {total_regular - done_regular} äºº")
                if total_regular > 0:
                    st.progress(done_regular / total_regular)
            with c2:
                st.metric("æ©Ÿå‹•/çµ„é•·å®Œæˆç‡", f"{done_mobile}/{total_mobile}", delta=f"å°šç¼º {total_mobile - done_mobile} äºº")
                if total_mobile > 0:
                    st.progress(done_mobile / total_mobile)

            st.divider()

            # 5. é¡¯ç¤ºæœªå®Œæˆåå–® (å·¦å³ä¸¦åˆ—)
            col_reg, col_mob = st.columns(2)
            
            with col_reg:
                st.write("#### ğŸ”´ ç­ç´šè©•åˆ†å“¡ (æœªå®Œæˆ)")
                missing_reg = [x for x in regular_inspectors if not x["done"]]
                if missing_reg:
                    for p in missing_reg:
                        st.error(f"âŒ {p['name']}")
                else:
                    st.success("ğŸ‰ å…¨å“¡å®Œæˆï¼")

                with st.expander("æŸ¥çœ‹å·²å®Œæˆåå–®"):
                    for p in regular_inspectors:
                        if p["done"]: st.write(f"âœ… {p['name']}")

            with col_mob:
                st.write("#### ğŸŸ  æ©Ÿå‹•/çµ„é•· (æœªå®Œæˆ)")
                st.caption("æ©Ÿå‹•äººå“¡è‹¥ä»Šæ—¥ç„¡é•è¦éœ€ç™»è¨˜ï¼Œå¯èƒ½ä¹Ÿä¸æœƒé€å‡ºè³‡æ–™ï¼Œè«‹æ–Ÿé…Œåƒè€ƒã€‚")
                missing_mob = [x for x in mobile_inspectors if not x["done"]]
                if missing_mob:
                    for p in missing_mob:
                        # æ©Ÿå‹•çµ„é¡¯ç¤ºè² è²¬é …ç›®ï¼Œæ–¹ä¾¿çµ„é•·åˆ¤æ–·ä»–ä»Šå¤©æ˜¯ä¸æ˜¯çœŸçš„æ²’äº‹
                        st.warning(f"âš ï¸ {p['name']} \n   (è² è²¬: {p['role_desc']})")
                else:
                    st.success("ğŸ‰ å…¨å“¡å®Œæˆï¼")

                with st.expander("æŸ¥çœ‹å·²å®Œæˆåå–®"):
                    for p in mobile_inspectors:
                        if p["done"]: st.write(f"âœ… {p['name']}")

        with tab1: # æˆç¸¾ç¸½è¡¨
            st.subheader("æˆç¸¾ç¸½è¡¨")
            df = load_main_data()
            all_classes_df = pd.DataFrame(all_classes, columns=["ç­ç´š"])
            if not df.empty:
                valid_weeks = sorted(df[df["é€±æ¬¡"]>0]["é€±æ¬¡"].unique())
                selected_weeks = st.multiselect("é¸æ“‡é€±æ¬¡", valid_weeks, default=valid_weeks[-1:] if valid_weeks else [], key='week_select_summary')
                if selected_weeks:
                    wdf = df[df["é€±æ¬¡"].isin(selected_weeks)].copy()
                    daily_agg = wdf.groupby(["æ—¥æœŸ", "ç­ç´š"]).agg({
                        "å…§æƒåŸå§‹åˆ†": "sum", "å¤–æƒåŸå§‹åˆ†": "sum", "åƒåœ¾åŸå§‹åˆ†": "sum",
                        "æ™¨é–“æ‰“æƒåŸå§‹åˆ†": "sum", "æ‰‹æ©Ÿäººæ•¸": "sum"
                    }).reset_index()
                    daily_agg["å…§æƒçµç®—"] = daily_agg["å…§æƒåŸå§‹åˆ†"].apply(lambda x: min(x, 2))
                    daily_agg["å¤–æƒçµç®—"] = daily_agg["å¤–æƒåŸå§‹åˆ†"].apply(lambda x: min(x, 2))
                    daily_agg["åƒåœ¾çµç®—"] = daily_agg["åƒåœ¾åŸå§‹åˆ†"].apply(lambda x: min(x, 2))
                    daily_agg["æ¯æ—¥ç¸½æ‰£åˆ†"] = (daily_agg["å…§æƒçµç®—"] + daily_agg["å¤–æƒçµç®—"] + 
                                            daily_agg["åƒåœ¾çµç®—"] + daily_agg["æ™¨é–“æ‰“æƒåŸå§‹åˆ†"] + daily_agg["æ‰‹æ©Ÿäººæ•¸"])
                    violation_report = daily_agg.groupby("ç­ç´š").agg({
                        "å…§æƒçµç®—": "sum", "å¤–æƒçµç®—": "sum", "åƒåœ¾çµç®—": "sum",
                        "æ™¨é–“æ‰“æƒåŸå§‹åˆ†": "sum", "æ‰‹æ©Ÿäººæ•¸": "sum", "æ¯æ—¥ç¸½æ‰£åˆ†": "sum"
                    }).reset_index()
                    violation_report.columns = ["ç­ç´š", "å…§æƒæ‰£åˆ†", "å¤–æƒæ‰£åˆ†", "åƒåœ¾æ‰£åˆ†", "æ™¨æƒæ‰£åˆ†", "æ‰‹æ©Ÿæ‰£åˆ†", "ç¸½æ‰£åˆ†"]
                    final_report = pd.merge(all_classes_df, violation_report, on="ç­ç´š", how="left").fillna(0)
                    final_report["ç¸½æˆç¸¾"] = 90 - final_report["ç¸½æ‰£åˆ†"]
                    final_report = final_report.sort_values("ç¸½æˆç¸¾", ascending=False)
                    st.dataframe(final_report, column_config={
                        "ç¸½æˆç¸¾": st.column_config.ProgressColumn("ç¸½æˆç¸¾", format="%d", min_value=60, max_value=90),
                        "ç¸½æ‰£åˆ†": st.column_config.NumberColumn("ç¸½æ‰£åˆ†", format="%d åˆ†")
                    }, use_container_width=True)
                    csv = final_report.to_csv(index=False).encode('utf-8-sig')
                    st.download_button("ğŸ“¥ ä¸‹è¼‰ (CSV)", csv, f"report_weeks_{selected_weeks}.csv")
                else: st.info("è«‹é¸æ“‡é€±æ¬¡")
            else: st.warning("ç„¡è³‡æ–™")

        with tab2: # è©³ç´°æ˜ç´°
            st.subheader("ğŸ“ é•è¦è©³ç´°æµæ°´å¸³")
            df = load_main_data()
            if not df.empty:
                valid_weeks = sorted(df[df["é€±æ¬¡"]>0]["é€±æ¬¡"].unique())
                s_weeks = st.multiselect("é¸æ“‡é€±æ¬¡", valid_weeks, default=valid_weeks[-1:] if valid_weeks else [], key='week_select_detail')
                if s_weeks:
                    detail_df = df[df["é€±æ¬¡"].isin(s_weeks)].copy()
                    detail_df["è©²ç­†æ‰£åˆ†"] = detail_df["å…§æƒåŸå§‹åˆ†"] + detail_df["å¤–æƒåŸå§‹åˆ†"] + detail_df["åƒåœ¾åŸå§‹åˆ†"] + detail_df["æ™¨é–“æ‰“æƒåŸå§‹åˆ†"] + detail_df["æ‰‹æ©Ÿäººæ•¸"]
                    detail_df = detail_df[detail_df["è©²ç­†æ‰£åˆ†"] > 0]
                    display_cols = ["æ—¥æœŸ", "ç­ç´š", "è©•åˆ†é …ç›®", "è©²ç­†æ‰£åˆ†", "å‚™è¨»", "æª¢æŸ¥äººå“¡", "é•è¦ç´°é …", "ç´€éŒ„ID"]
                    detail_df = detail_df[display_cols].sort_values(["æ—¥æœŸ", "ç­ç´š"])
                    st.dataframe(detail_df, use_container_width=True)
                    csv_detail = detail_df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button("ğŸ“¥ ä¸‹è¼‰ (CSV)", csv_detail, f"detail_log_{s_weeks}.csv")
                else: st.info("è«‹é¸æ“‡é€±æ¬¡")
            else: st.info("ç„¡è³‡æ–™")

        with tab3: # å¯„é€é€šçŸ¥
            st.subheader("ğŸ“§ æ¯æ—¥é•è¦é€šçŸ¥")
            target_date = st.date_input("é¸æ“‡æ—¥æœŸ", today_tw)
            if "mail_preview" not in st.session_state: st.session_state.mail_preview = None
            if st.button("ğŸ” çµ±æ•´ç•¶æ—¥é•è¦"):
                df = load_main_data()
                try:
                    df["æ—¥æœŸObj"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce').dt.date
                    day_df = df[df["æ—¥æœŸObj"] == target_date]
                except: day_df = pd.DataFrame()
                if not day_df.empty:
                    stats = day_df.groupby("ç­ç´š")[["å…§æƒåŸå§‹åˆ†", "å¤–æƒåŸå§‹åˆ†", "åƒåœ¾åŸå§‹åˆ†", "æ™¨é–“æ‰“æƒåŸå§‹åˆ†", "æ‰‹æ©Ÿäººæ•¸"]].sum().reset_index()
                    stats["å…§æƒ"] = stats["å…§æƒåŸå§‹åˆ†"].clip(upper=2)
                    stats["å¤–æƒ"] = stats["å¤–æƒåŸå§‹åˆ†"].clip(upper=2)
                    stats["åƒåœ¾"] = stats["åƒåœ¾åŸå§‹åˆ†"].clip(upper=2)
                    stats["ç•¶æ—¥ç¸½æ‰£åˆ†"] = stats["å…§æƒ"] + stats["å¤–æƒ"] + stats["åƒåœ¾"] + stats["æ™¨é–“æ‰“æƒåŸå§‹åˆ†"] + stats["æ‰‹æ©Ÿäººæ•¸"]
                    violation_classes = stats[stats["ç•¶æ—¥ç¸½æ‰£åˆ†"] > 0]
                    if not violation_classes.empty:
                        preview_data = []
                        for _, row in violation_classes.iterrows():
                            cls_name = row["ç­ç´š"]
                            t_info = TEACHER_MAILS.get(cls_name, {})
                            t_name = t_info.get('name', "âŒ ç¼ºåå–®")
                            t_email = t_info.get('email', "âŒ ç„¡æ³•å¯„é€")
                            status = "æº–å‚™å¯„é€" if "@" in t_email else "ç•°å¸¸"
                            preview_data.append({"ç­ç´š": cls_name, "ç•¶æ—¥ç¸½æ‰£åˆ†": row["ç•¶æ—¥ç¸½æ‰£åˆ†"], "å°å¸«å§“å": t_name, "æ”¶ä»¶ä¿¡ç®±": t_email, "ç‹€æ…‹": status})
                        st.session_state.mail_preview = pd.DataFrame(preview_data)
                        st.success(f"æ‰¾åˆ° {len(violation_classes)} ç­†é•è¦ç­ç´š")
                    else: st.session_state.mail_preview = None; st.info("ä»Šæ—¥ç„¡é•è¦")
                else: st.session_state.mail_preview = None; st.info("ä»Šæ—¥ç„¡è³‡æ–™")

            if st.session_state.mail_preview is not None:
                st.write("### ğŸ“¨ å¯„é€é è¦½æ¸…å–®"); st.dataframe(st.session_state.mail_preview)
                if st.button("ğŸš€ ä¸€éµå¯„å‡ºï¼"):
                    mail_queue_list = []
                    for _, row in st.session_state.mail_preview.iterrows():
                        if row["ç‹€æ…‹"] == "æº–å‚™å¯„é€":
                            subject = f"ğŸ””({target_date})è¡›ç”Ÿçµ„è©•åˆ†å ±è¡¨ - {row['ç­ç´š']}"
                            content = f"è€å¸«æ‚¨å¥½ï¼š\n\né€™æ˜¯ä¾†è‡ªè¡›ç”Ÿçµ„ç³»çµ±è‡ªå‹•ç™¼é€ä¹‹æ¯æ—¥å ±è¡¨ã€‚\nä¾æ“šä»Šæ—¥({target_date}) çš„è©•åˆ†è¨˜éŒ„\nè¡›ç”Ÿè©•åˆ†ç¸½æ‰£åˆ†ç‚ºï¼š{row['ç•¶æ—¥ç¸½æ‰£åˆ†']} åˆ†ã€‚\né€™é‚Šè¦æ‹œè¨—è€å¸«é¼“å‹µåŠæé†’è² è²¬å­¸ç”Ÿï¼Œä¸€èµ·ä¾†ç‚ºå­¸æ ¡çš„ç’°å¢ƒåŠªåŠ›ä¸€ä¸‹ğŸª„\nçœŸå¿ƒæ„Ÿæ©è¾›è‹¦çš„å°å¸«\n\nå¦‚æœ‰ç–‘å•å¯è‡³è¡›ç”Ÿçµ„è©•åˆ†ç³»çµ±æŸ¥è©¢æ‰£åˆ†ç´°ç¯€å“¦!\n https://clvshygiene.streamlit.app/ \n\nå­¸å‹™è™•è¡›ç”Ÿçµ„æ•¬ä¸Š"
                            mail_queue_list.append({'email': row["æ”¶ä»¶ä¿¡ç®±"], 'subject': subject, 'body': content})
                    
                    if mail_queue_list:
                        with st.spinner("ğŸ“§ æ­£åœ¨å»ºç«‹ SMTP é€£ç·šä¸¦æ‰¹æ¬¡å¯„é€..."):
                            count, msg = send_bulk_emails(mail_queue_list)
                            if count > 0: st.success(f"âœ… æˆåŠŸå¯„å‡º {count} å°ä¿¡ä»¶ï¼ ({msg})")
                            else: st.error(f"âŒ å¯„é€å¤±æ•—: {msg}")
                        st.session_state.mail_preview = None
                    else: st.warning("æ²’æœ‰å¯å¯„é€çš„å°è±¡")

        with tab4: # ç”³è¨´å¯©æ ¸
            st.subheader("ğŸ“£ ç”³è¨´æ¡ˆä»¶å¯©æ ¸")
            appeals_df = load_appeals()
            pending = appeals_df[appeals_df["è™•ç†ç‹€æ…‹"] == "å¾…è™•ç†"]
            if not pending.empty:
                st.info(f"å¾…å¯©æ ¸: {len(pending)} ä»¶")
                for idx, row in pending.iterrows():
                    with st.container(border=True):
                        c1, c2 = st.columns([2, 1])
                        with c1:
                            st.markdown(f"**{row['ç­ç´š']}** | {row['é•è¦é …ç›®']} | æ‰£ {row['åŸå§‹æ‰£åˆ†']} åˆ†")
                            st.markdown(f"ç†ç”±ï¼š{row['ç”³è¨´ç†ç”±']}")
                        with c2:
                            url = row.get("ä½è­‰ç…§ç‰‡", "")
                            if url and url != "UPLOAD_FAILED": st.image(url, width=150)
                        b1, b2 = st.columns(2)
                        if b1.button("âœ… æ ¸å¯", key=f"ok_{idx}"):
                            succ, msg = update_appeal_status(idx, "å·²æ ¸å¯", row["å°æ‡‰ç´€éŒ„ID"])
                            if succ: st.success("å·²æ ¸å¯"); time.sleep(1); st.rerun()
                        
                        if b2.button("ğŸš« é§å›", key=f"ng_{idx}"):
                            succ, msg = update_appeal_status(idx, "å·²é§å›", row["å°æ‡‰ç´€éŒ„ID"])
                            if succ: st.warning("å·²é§å›"); time.sleep(1); st.rerun()
            else: st.success("ç„¡å¾…å¯©æ ¸æ¡ˆä»¶")
            with st.expander("æ­·å²æ¡ˆä»¶"): st.dataframe(appeals_df[appeals_df["è™•ç†ç‹€æ…‹"] != "å¾…è™•ç†"])

        with tab5: # ç³»çµ±è¨­å®š
            st.subheader("âš™ï¸ ç³»çµ±è¨­å®š")
            curr = SYSTEM_CONFIG.get("semester_start", "2025-08-25")
            nd = st.date_input("é–‹å­¸æ—¥", datetime.strptime(curr, "%Y-%m-%d").date())
            if st.button("æ›´æ–°é–‹å­¸æ—¥"): save_setting("semester_start", str(nd)); st.success("å·²æ›´æ–°")
            st.divider()
            st.markdown("### ğŸ—‘ï¸ è³‡æ–™ç¶­è­·")
            df = load_main_data()
            if not df.empty:
                del_mode = st.radio("åˆªé™¤æ¨¡å¼", ["å–®ç­†åˆªé™¤", "æ—¥æœŸå€é–“åˆªé™¤"])
                if del_mode == "å–®ç­†åˆªé™¤":
                    df_display = df.sort_values("ç™»éŒ„æ™‚é–“", ascending=False).head(50)
                    opts = {r['ç´€éŒ„ID']: f"{r['æ—¥æœŸ']} | {r['ç­ç´š']} | {r['è©•åˆ†é …ç›®']} (ID:{r['ç´€éŒ„ID']})" for _, r in df_display.iterrows()}
                    sel_ids = st.multiselect("é¸æ“‡è¦åˆªé™¤çš„ç´€éŒ„", list(opts.keys()), format_func=lambda x: opts[x], key='del_multiselect')
                    if st.button("ğŸ—‘ï¸ ç¢ºèªåˆªé™¤"):
                        if delete_rows_by_ids(sel_ids): st.success("åˆªé™¤æˆåŠŸ"); st.rerun()
                elif del_mode == "æ—¥æœŸå€é–“åˆªé™¤":
                    c1, c2 = st.columns(2)
                    d_start = c1.date_input("é–‹å§‹"); d_end = c2.date_input("çµæŸ")
                    if st.button("âš ï¸ ç¢ºèªåˆªé™¤å€é–“è³‡æ–™"):
                        df["d_tmp"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce').dt.date
                        target_ids = df[(df["d_tmp"] >= d_start) & (df["d_tmp"] <= d_end)]["ç´€éŒ„ID"].tolist()
                        if target_ids:
                            if delete_rows_by_ids(target_ids): st.success(f"å·²åˆªé™¤ {len(target_ids)} ç­†"); st.rerun()
                        else: st.warning("ç„¡è³‡æ–™")
            else: st.info("ç„¡è³‡æ–™")

        with tab6:
            st.info("è«‹è‡³ Google Sheets ä¿®æ”¹åå–®")
            if st.button("ğŸ”„ é‡æ–°è®€å–å¿«å–"): st.cache_data.clear(); st.success("OK")
            st.markdown(f"[é–‹å•Ÿè©¦ç®—è¡¨]({SHEET_URL})")

        with tab7: # æ™¨æƒç®¡ç†
            st.subheader("ğŸ§¹ æ™¨æƒè©•åˆ†")
            m_date = st.date_input("æ—¥æœŸ", today_tw, key="m_d")
            m_week = get_week_num(m_date)
            duty_list, status = get_daily_duty(m_date)
            if status == "success":
                st.write(f"æ‡‰åˆ°: {len(duty_list)} äºº")
                with st.form("m_form"):
                    edited = st.data_editor(pd.DataFrame(duty_list), hide_index=True, use_container_width=True)
                    score = st.number_input("æ‰£åˆ†", min_value=1, value=1)
                    if st.form_submit_button("é€å‡º"):
                        base = {"æ—¥æœŸ": m_date, "é€±æ¬¡": m_week, "æª¢æŸ¥äººå“¡": "è¡›ç”Ÿçµ„", "ç™»éŒ„æ™‚é–“": now_tw.strftime("%Y-%m-%d %H:%M:%S"), "ä¿®æ­£": False}
                        cnt = 0
                        for _, r in edited[edited["å·²å®Œæˆæ‰“æƒ"] == False].iterrows():
                            tid = clean_id(r["å­¸è™Ÿ"])
                            cls = ROSTER_DICT.get(tid, f"æŸ¥ç„¡({tid})")
                            save_entry({**base, "ç­ç´š": cls, "è©•åˆ†é …ç›®": "æ™¨é–“æ‰“æƒ", "æ™¨é–“æ‰“æƒåŸå§‹åˆ†": score, "å‚™è¨»": f"æœªåˆ°-å­¸è™Ÿ:{tid}", "æ™¨æƒæœªåˆ°è€…": tid})
                            cnt += 1
                        st.success(f"å·²æ’å…¥èƒŒæ™¯ï¼š{cnt} äºº"); st.rerun()
            else: st.warning(f"ç„¡è¼ªå€¼è³‡æ–™ ({status})")

    else: st.error("å¯†ç¢¼éŒ¯èª¤")